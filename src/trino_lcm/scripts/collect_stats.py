"""
Trinoã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®çµ±è¨ˆæƒ…å ±ã‚’åé›†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Usage:
    # å˜ä¸€ã‚¹ã‚­ãƒ¼ãƒã®çµ±è¨ˆæƒ…å ±ã‚’åé›†
    python src/trino_lcm/scripts/collect_stats.py \
        --catalog iceberg \
        --schema imdb \
        --tables name,cast_info
    
    # ã¾ãŸã¯å…¨ãƒ†ãƒ¼ãƒ–ãƒ«
    python src/trino_lcm/scripts/collect_stats.py \
        --catalog iceberg \
        --schema accidents
    
    # è¤‡æ•°ã‚¹ã‚­ãƒ¼ãƒã‚’æŒ‡å®š
    python src/trino_lcm/scripts/collect_stats.py \
        --catalog iceberg \
        --schemas accidents,airline,imdb
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆæƒ…å ±ã‚’ä¸€æ‹¬åé›†
    python src/trino_lcm/scripts/collect_stats.py \
        --catalog iceberg \
        --all-schemas
    
çµ±è¨ˆæƒ…å ±ã¯ä»¥ä¸‹ã®æ§‹é€ ã§ä¿å­˜ã•ã‚Œã¾ã™:
    datasets_statistics/
        <catalog>_<schema>/
            column_stats.json  # ã‚«ãƒ©ãƒ ãƒ¬ãƒ™ãƒ«ã®çµ±è¨ˆæƒ…å ±ï¼ˆPostgreSQLäº’æ›ï¼‰
            table_stats.json   # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ¬ãƒ™ãƒ«ã®çµ±è¨ˆæƒ…å ±ï¼ˆPostgreSQLäº’æ›ï¼‰
            metadata.json      # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
"""

import argparse
import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import re

# åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåï¼ˆå…¨ã‚¹ã‚­ãƒ¼ãƒå‡¦ç†æ™‚ã«ä½¿ç”¨ï¼‰
AVAILABLE_DATASETS = [
    'accidents',
    'airline',
    'baseball',
    'basketball',
    'carcinogenesis',
    'consumer',
    'credit',
    'employee',
    'fhnk',
    'financial',
    'geneea',
    'genome',
    'hepatitis',
    'imdb',
    'movielens',
    'seznam',
    'ssb',
    'tournament',
    'tpc_h',
    'walmart'
]


def run_trino_query(catalog: str, schema: str, query: str) -> List[List[str]]:
    """Trinoã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™"""
    try:
        # Trinoã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨
        from trino.dbapi import connect
        
        # æ¥ç¶šè¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—ï¼‰
        host = os.getenv('TRINO_HOST', 'localhost')
        port = int(os.getenv('TRINO_PORT', '8080'))
        user = os.getenv('TRINO_USER', 'admin')
        
        conn = connect(
            host=host,
            port=port,
            user=user,
            catalog=catalog,
            schema=schema
        )
        
        cursor = conn.cursor()
        cursor.execute(query)
        
        # çµæœã‚’å–å¾—
        rows = cursor.fetchall()
        
        # å…ƒã®å‹ã‚’ä¿æŒï¼ˆæ–‡å­—åˆ—å¤‰æ›ã¯ã—ãªã„ï¼‰
        # Noneå€¤ã¯ãã®ã¾ã¾ä¿æŒã—ã€å¾Œã®å‡¦ç†ã§é©åˆ‡ã«å‡¦ç†ã™ã‚‹
        result = []
        for row in rows:
            result.append(list(row))  # å…ƒã®å‹ã‚’ä¿æŒ
        
        cursor.close()
        conn.close()
        
        return result
        
    except ImportError:
        print("âŒ trinoãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“", file=sys.stderr)
        print("   pip install trino ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„", file=sys.stderr)
        return []
    except Exception as e:
        print(f"âŒ Trinoæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        return []


def quote_identifier(identifier: str) -> str:
    """
    Trinoã®è­˜åˆ¥å­ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«åã€ã‚«ãƒ©ãƒ åï¼‰ã‚’é©åˆ‡ã«ã‚¯ã‚©ãƒ¼ãƒˆã™ã‚‹
    
    Trinoã§ã¯ä»¥ä¸‹ã®å ´åˆã«ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆãŒå¿…è¦ï¼š
    - æ•°å­—ã§å§‹ã¾ã‚‹
    - ç‰¹åˆ¥æ–‡å­—ã‚’å«ã‚€
    - äºˆç´„èª
    - å¤§æ–‡å­—ã¨å°æ–‡å­—ã‚’åŒºåˆ¥ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
    
    Returns:
        ã‚¯ã‚©ãƒ¼ãƒˆã•ã‚ŒãŸè­˜åˆ¥å­
    """
    # æ—¢ã«ã‚¯ã‚©ãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã®ã¾ã¾è¿”ã™
    if identifier.startswith('"') and identifier.endswith('"'):
        return identifier
    
    # Trinoã®ä¸»è¦ãªäºˆç´„èªãƒªã‚¹ãƒˆ
    # ä¸€èˆ¬çš„ãªSQLäºˆç´„èªã¨Trinoå›ºæœ‰ã®äºˆç´„èªã‚’å«ã‚€
    trino_reserved_words = {
        'order', 'group', 'select', 'from', 'where', 'having', 'limit', 'offset',
        'join', 'inner', 'outer', 'left', 'right', 'full', 'cross', 'on', 'using',
        'union', 'intersect', 'except', 'all', 'distinct',
        'as', 'and', 'or', 'not', 'in', 'exists', 'between', 'is', 'null',
        'case', 'when', 'then', 'else', 'end',
        'cast', 'create', 'drop', 'alter', 'table', 'view', 'index',
        'insert', 'update', 'delete', 'values',
        'grant', 'revoke', 'commit', 'rollback', 'transaction',
        'primary', 'key', 'foreign', 'references', 'constraint', 'default',
        'set', 'show', 'describe', 'explain', 'use', 'database', 'schema',
        'like', 'ilike', 'similar', 'over', 'partition', 'window',
        'lateral', 'array', 'map', 'row', 'timestamp', 'date', 'time', 'interval',
        'asc', 'desc', 'any', 'some', 'by', 'with', 'recursive'
    }
    
    # å°æ–‡å­—ã«å¤‰æ›ã—ã¦äºˆç´„èªãƒã‚§ãƒƒã‚¯
    identifier_lower = identifier.lower()
    
    # æ•°å­—ã§å§‹ã¾ã‚‹ã€ã¾ãŸã¯ç‰¹åˆ¥æ–‡å­—ã‚’å«ã‚€å ´åˆã¯ã‚¯ã‚©ãƒ¼ãƒˆ
    if re.match(r'^\d', identifier) or not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', identifier):
        return f'"{identifier}"'
    
    # äºˆç´„èªã®å ´åˆã¯ã‚¯ã‚©ãƒ¼ãƒˆ
    if identifier_lower in trino_reserved_words:
        return f'"{identifier}"'
    
    return identifier


def get_table_list(catalog: str, schema: str) -> List[str]:
    """ã‚¹ã‚­ãƒ¼ãƒå†…ã®ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    query = "SHOW TABLES"
    rows = run_trino_query(catalog, schema, query)
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
    if rows and rows[0][0].lower() == 'table':
        rows = rows[1:]
    
    return [row[0] for row in rows if row]


def get_table_stats(catalog: str, schema: str, table: str) -> Dict[str, Any]:
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    quoted_table = quote_identifier(table)
    query = f"SHOW STATS FOR {quoted_table}"
    rows = run_trino_query(catalog, schema, query)
    
    if not rows:
        return {}
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆæœ€åˆã®è¡Œï¼‰
    header = rows[0]
    data_rows = rows[1:]
    
    # çµ±è¨ˆæƒ…å ±ã‚’æ§‹é€ åŒ–
    stats = {
        'table_name': table,
        'row_count': None,
        'columns': {}
    }
    
    for row in data_rows:
        if len(row) < len(header):
            continue
        
        # ã‚«ãƒ©ãƒ åã‚’å–å¾—ï¼ˆæ–‡å­—åˆ—ã«å¤‰æ›ï¼‰
        column_name = str(row[0]) if row[0] is not None else None
        
        # ã‚«ãƒ©ãƒ åãŒNULLã®è¡Œã¯å…¨ä½“çµ±è¨ˆ
        if not column_name or column_name.upper() == 'NULL':
            # row_countã‚’å–å¾—
            if len(row) >= 5 and row[4] is not None:
                try:
                    # æ•°å€¤å‹ã®å ´åˆã¯ãã®ã¾ã¾ã€æ–‡å­—åˆ—ã®å ´åˆã¯å¤‰æ›
                    if isinstance(row[4], (int, float)):
                        stats['row_count'] = float(row[4])
                    elif isinstance(row[4], str) and row[4].upper() != 'NULL':
                        stats['row_count'] = float(row[4])
                except (ValueError, TypeError):
                    pass
            continue
        
        # ã‚«ãƒ©ãƒ çµ±è¨ˆã‚’æ§‹é€ åŒ–
        col_stats = {
            'column_name': column_name,
            'data_size': parse_float(row[1]) if len(row) > 1 else None,
            'distinct_values_count': parse_float(row[2]) if len(row) > 2 else None,
            'nulls_fraction': parse_float(row[3]) if len(row) > 3 else None,
            'low_value': str(row[5]) if len(row) > 5 and row[5] is not None else None,
            'high_value': str(row[6]) if len(row) > 6 and row[6] is not None else None,
        }
        
        stats['columns'][column_name] = col_stats
    
    return stats


def parse_float(value: Any) -> Optional[float]:
    """å€¤ã‚’floatã«å¤‰æ›ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰"""
    if value is None:
        return None
    
    # æ—¢ã«æ•°å€¤å‹ã®å ´åˆ
    if isinstance(value, (int, float)):
        return float(value)
    
    # æ–‡å­—åˆ—ã®å ´åˆ
    if isinstance(value, str):
        if not value or value.upper() == 'NULL':
            return None
        try:
            return float(value)
        except ValueError:
            return None
    
    # ãã®ä»–ã®å‹
    return None


def estimate_pages(row_count: float, avg_row_size: int = 100) -> int:
    """
    è¡Œæ•°ã‹ã‚‰ãƒšãƒ¼ã‚¸æ•°ã‚’æ¨å®š
    PostgreSQLã®relpagesäº’æ›å€¤ã‚’ç”Ÿæˆ
    """
    if row_count == 0:
        return 0
    
    # PostgreSQLã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºã¯8KB
    page_size = 8192
    # æ¨å®šç·ã‚µã‚¤ã‚ºï¼ˆãƒã‚¤ãƒˆï¼‰
    estimated_size = row_count * avg_row_size
    # ãƒšãƒ¼ã‚¸æ•°ã‚’è¨ˆç®—
    pages = max(1, int(estimated_size / page_size))
    
    return pages


def get_column_percentiles(catalog: str, schema: str, table: str, column: str) -> Optional[List[float]]:
    """
    ã‚«ãƒ©ãƒ ã®ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«å€¤ï¼ˆ11å€‹: 0.0, 0.1, ..., 1.0ï¼‰ã‚’å–å¾—
    
    æ³¨æ„: Trinoã®approx_percentile()é–¢æ•°ãŒä½¿ç”¨ã§ããªã„ç’°å¢ƒã‚‚ã‚ã‚‹ãŸã‚ã€
    ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’å¼·åŒ–ã—ã¦ã„ã¾ã™ã€‚
    
    Returns:
        ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«å€¤ã®ãƒªã‚¹ãƒˆã€å¤±æ•—æ™‚ã¯None
    """
    try:
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ã‚«ãƒ©ãƒ åã‚’ã‚¯ã‚©ãƒ¼ãƒˆ
        quoted_table = quote_identifier(table)
        quoted_column = quote_identifier(column)
        
        # ã¾ãšapprox_percentile()ãŒä½¿ç”¨å¯èƒ½ã‹ãƒ†ã‚¹ãƒˆ
        test_query = f"SELECT approx_percentile({quoted_column}, 0.5) FROM {quoted_table} LIMIT 1"
        test_rows = run_trino_query(catalog, schema, test_query)
        
        if not test_rows or not test_rows[0] or test_rows[0][0] is None:
            print(f"      âš ï¸ approx_percentile()é–¢æ•°ãŒä½¿ç”¨ã§ãã¾ã›ã‚“ï¼ˆå€¤ãŒNULLã¾ãŸã¯çµæœãªã—ï¼‰")
            return None
        
        # 11å€‹ã®ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«å€¤ã‚’ä¸€åº¦ã«å–å¾—
        percentiles = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
        percentile_queries = [
            f"approx_percentile({quoted_column}, {p})" for p in percentiles
        ]
        query = f"SELECT {', '.join(percentile_queries)} FROM {quoted_table} LIMIT 1"
        
        rows = run_trino_query(catalog, schema, query)
        
        if rows and len(rows) > 0 and len(rows[0]) == 11:
            # çµæœã‚’floatã®ãƒªã‚¹ãƒˆã«å¤‰æ›
            result = []
            for val in rows[0]:
                try:
                    # Noneå€¤ã¾ãŸã¯'NULL'æ–‡å­—åˆ—ã‚’ãƒã‚§ãƒƒã‚¯
                    if val is None:
                        print(f"      âš ï¸ ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«å€¤ã®ä¸€éƒ¨ãŒNone")
                        return None
                    
                    # æ–‡å­—åˆ—ã¨ã—ã¦'NULL'ã®å ´åˆ
                    if isinstance(val, str) and val.upper() == 'NULL':
                        print(f"      âš ï¸ ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«å€¤ã®ä¸€éƒ¨ãŒNULL")
                        return None
                    
                    # å€¤ã‚’floatã«å¤‰æ›ï¼ˆ0.0ã‚‚æœ‰åŠ¹ãªå€¤ã¨ã—ã¦å‡¦ç†ï¼‰
                    result.append(float(val))
                except (ValueError, TypeError) as e:
                    print(f"      âš ï¸ ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«å€¤ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}, å€¤: {val} (å‹: {type(val)})")
                    return None
            return result
        else:
            expected_count = len(rows[0]) if rows and rows[0] else 0
            print(f"      âš ï¸ ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«å–å¾—çµæœã®å½¢å¼ãŒä¸æ­£: æœŸå¾…11å€‹ã€å®Ÿéš›{expected_count}å€‹")
            return None
    except Exception as e:
        print(f"      âš ï¸ approx_percentile()ã‚¨ãƒ©ãƒ¼: {e}")
        return None


def get_column_min_max(catalog: str, schema: str, table: str, column: str) -> Tuple[Optional[float], Optional[float]]:
    """
    ã‚«ãƒ©ãƒ ã®MIN/MAXå€¤ã‚’å–å¾—
    
    Returns:
        (min_value, max_value)
    """
    try:
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã¨ã‚«ãƒ©ãƒ åã‚’ã‚¯ã‚©ãƒ¼ãƒˆ
        quoted_table = quote_identifier(table)
        quoted_column = quote_identifier(column)
        query = f"SELECT MIN({quoted_column}), MAX({quoted_column}) FROM {quoted_table}"
        rows = run_trino_query(catalog, schema, query)
        
        if rows and len(rows) > 0 and len(rows[0]) >= 2:
            min_val = rows[0][0]
            max_val = rows[0][1]
            
            try:
                # å€¤ãŒæ—¢ã«æ•°å€¤å‹ã®å ´åˆï¼ˆrun_trino_queryãŒå…ƒã®å‹ã‚’ä¿æŒï¼‰
                if isinstance(min_val, (int, float)):
                    min_float = float(min_val)
                elif min_val is None:
                    min_float = None
                elif isinstance(min_val, str):
                    # æ–‡å­—åˆ—ãŒ'NULL'ã¾ãŸã¯ç©ºã®å ´åˆ
                    if min_val.upper() == 'NULL' or not min_val.strip():
                        min_float = None
                    else:
                        try:
                            min_float = float(min_val)
                        except ValueError:
                            min_float = None
                else:
                    # ãã®ä»–ã®å‹ã¯Noneã¨ã—ã¦æ‰±ã†
                    min_float = None
                
                # åŒæ§˜ã«max_valã‚’å‡¦ç†
                if isinstance(max_val, (int, float)):
                    max_float = float(max_val)
                elif max_val is None:
                    max_float = None
                elif isinstance(max_val, str):
                    if max_val.upper() == 'NULL' or not max_val.strip():
                        max_float = None
                    else:
                        try:
                            max_float = float(max_val)
                        except ValueError:
                            max_float = None
                else:
                    max_float = None
                
                return (min_float, max_float)
            except (ValueError, TypeError, AttributeError) as e:
                print(f"      âš ï¸ MIN/MAXå€¤ã®å¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}, min_val={min_val}, max_val={max_val}")
                return (None, None)
        return (None, None)
    except Exception as e:
        print(f"      âš ï¸ MIN/MAXå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return (None, None)


def get_column_data_type(catalog: str, schema: str, table: str, column: str) -> Optional[str]:
    """
    ã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—ï¼ˆIcebergãƒ†ãƒ¼ãƒ–ãƒ«ç”¨ï¼‰
    
    Icebergã®DESCRIBEå‡ºåŠ›å½¢å¼: [column_name, data_type, '', '']
    PostgreSQLã¨ã¯ç•°ãªã‚‹ã®ã§æ³¨æ„ãŒå¿…è¦
    
    Returns:
        ãƒ‡ãƒ¼ã‚¿å‹ï¼ˆbigint, double, varcharç­‰ï¼‰ã€å¤±æ•—æ™‚ã¯None
    """
    try:
        # ãƒ†ãƒ¼ãƒ–ãƒ«åã‚’ã‚¯ã‚©ãƒ¼ãƒˆ
        quoted_table = quote_identifier(table)
        query = f"DESCRIBE {quoted_table}"
        rows = run_trino_query(catalog, schema, query)
        
        for row in rows:
            # Icebergã®DESCRIBEå‡ºåŠ›ã¯ [column_name, data_type, '', ''] ã®å½¢å¼
            # ã‚«ãƒ©ãƒ åã¯æœ€åˆã®è¦ç´ ã€ãƒ‡ãƒ¼ã‚¿å‹ã¯2ç•ªç›®ã®è¦ç´ 
            if len(row) >= 2 and row[0] == column:
                data_type = row[1].strip() if row[1] else None
                return data_type
        
        return None
    except Exception as e:
        print(f"      âš ï¸ DESCRIBEã‚¨ãƒ©ãƒ¼: {e}")
        return None


def trino_to_pg_data_type(trino_type: Optional[str]) -> str:
    """
    Trinoã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’PostgreSQLäº’æ›ã®ãƒ‡ãƒ¼ã‚¿å‹ã«å¤‰æ›
    
    Args:
        trino_type: Trinoã®ãƒ‡ãƒ¼ã‚¿å‹ï¼ˆbigint, double, varcharç­‰ï¼‰
    
    Returns:
        PostgreSQLäº’æ›ã®ãƒ‡ãƒ¼ã‚¿å‹å
    """
    if not trino_type:
        return 'unknown'
    
    trino_type_lower = trino_type.lower()
    
    # æ•´æ•°å‹
    if trino_type_lower in ('bigint', 'integer', 'int', 'smallint', 'tinyint', 'boolean'):
        return 'integer'
    
    # æµ®å‹•å°æ•°ç‚¹å‹
    if trino_type_lower in ('double', 'real', 'float'):
        return 'double precision'
    
    # æ•°å€¤å‹ï¼ˆDECIMAL, NUMERICï¼‰
    if trino_type_lower in ('decimal', 'numeric'):
        return 'numeric'
    
    # æ—¥ä»˜ãƒ»æ™‚åˆ»å‹
    if trino_type_lower == 'date':
        return 'date'
    if trino_type_lower in ('timestamp', 'timestamp with time zone'):
        return 'timestamp'
    if trino_type_lower in ('time', 'time with time zone'):
        return 'time'
    
    # æ–‡å­—åˆ—å‹
    if trino_type_lower in ('varchar', 'char', 'string', 'text'):
        return 'character varying'
    
    # ãƒã‚¤ãƒŠãƒªå‹
    if trino_type_lower in ('varbinary', 'binary'):
        return 'bytea'
    
    # JSON/ARRAYç­‰ã®è¤‡åˆå‹
    if trino_type_lower in ('json', 'jsonb'):
        return 'json'
    if 'array' in trino_type_lower:
        return 'array'
    if 'map' in trino_type_lower:
        return 'map'
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šä¸æ˜ãªå‹ã¯æ–‡å­—åˆ—ã¨ã—ã¦æ‰±ã†
    return 'character varying'


def infer_data_type(col_stat: Dict[str, Any]) -> str:
    """
    ã‚«ãƒ©ãƒ çµ±è¨ˆã‹ã‚‰ ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ¨æ¸¬ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ï¼‰
    PostgreSQLäº’æ›ã®ãƒ‡ãƒ¼ã‚¿å‹åã‚’è¿”ã™
    
    æ³¨æ„: ã“ã®é–¢æ•°ã¯trino_data_typeãŒå–å¾—ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã§ã™
    """
    low = col_stat.get('low_value')
    high = col_stat.get('high_value')
    
    # å€¤ãŒå­˜åœ¨ã—ãªã„å ´åˆ
    if low is None and high is None:
        return 'unknown'
    
    # æ•°å€¤å‹ã®åˆ¤å®š
    if low is not None and high is not None:
        try:
            float(low)
            float(high)
            # æ•´æ•°ã‹ã©ã†ã‹åˆ¤å®š
            if '.' not in str(low) and '.' not in str(high):
                return 'integer'
            return 'double precision'
        except (ValueError, TypeError):
            pass
    
    # æ—¥ä»˜å‹ã®åˆ¤å®š
    if low and isinstance(low, str):
        # YYYY-MM-DDå½¢å¼
        if re.match(r'\d{4}-\d{2}-\d{2}', low):
            return 'date'
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—å½¢å¼
        if re.match(r'\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}', low):
            return 'timestamp'
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯æ–‡å­—åˆ—å‹
    return 'character varying'


def collect_schema_stats(
    catalog: str,
    schema: str,
    tables: Optional[List[str]],
    output_dir_base: Path
) -> Tuple[bool, Dict[str, Any], Dict[str, Any]]:
    """
    å˜ä¸€ã‚¹ã‚­ãƒ¼ãƒã®çµ±è¨ˆæƒ…å ±ã‚’åé›†
    
    Returns:
        (æˆåŠŸãƒ•ãƒ©ã‚°, ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆ, ã‚«ãƒ©ãƒ çµ±è¨ˆ)
    """
    # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
    if tables:
        table_list = tables
        print(f"ğŸ“‹ æŒ‡å®šã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(table_list)}")
    else:
        print("ğŸ“‹ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—ä¸­...")
        table_list = get_table_list(catalog, schema)
        if not table_list:
            print(f"   âš ï¸ ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return False, {}, {}
        print(f"   è¦‹ã¤ã‹ã£ãŸãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(table_list)}")
    
    print()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    schema_dir_name = f"{catalog}_{schema}"
    schema_output_dir = output_dir_base / schema_dir_name
    schema_output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {schema_output_dir}")
    print()
    
    # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®çµ±è¨ˆæƒ…å ±ã‚’åé›†
    table_stats = {}
    column_stats = {}
    
    for table in table_list:
        print(f"ğŸ“Š {table} ã®çµ±è¨ˆæƒ…å ±ã‚’åé›†ä¸­...")
        stats = get_table_stats(catalog, schema, table)
        
        if stats:
            row_count = stats.get('row_count', 0) or 0
            
            table_stats[table] = {
                'reltuples': row_count,
                'relpages': estimate_pages(row_count),
                'table_name': table,
                'row_count': row_count,
                'num_columns': len(stats.get('columns', {}))
            }
            
            # ã‚«ãƒ©ãƒ çµ±è¨ˆã‚’ä¿å­˜
            for col_name, col_stat in stats.get('columns', {}).items():
                tuple_key = f"('{table}', '{col_name}')"
                
                distinct_count = col_stat.get('distinct_values_count')
                null_frac = col_stat.get('nulls_fraction', 0) or 0
                data_size = col_stat.get('data_size')
                
                # ãƒ‡ãƒ¼ã‚¿å‹ã‚’å–å¾—
                data_type_raw = get_column_data_type(catalog, schema, table, col_name)
                
                # PostgreSQLäº’æ›ã®å‹ã«å¤‰æ›ï¼ˆå„ªå…ˆé †ä½ï¼štrino_data_type > æ¨è«–ï¼‰
                if data_type_raw:
                    inferred_type = trino_to_pg_data_type(data_type_raw)
                else:
                    # fallback: çµ±è¨ˆæƒ…å ±ã‹ã‚‰æ¨æ¸¬
                    inferred_type = infer_data_type(col_stat)
                
                # QueryFormerç”¨ã®ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›
                queryformer_datatype = None
                if data_type_raw:
                    if data_type_raw in ('bigint', 'integer', 'int'):
                        queryformer_datatype = 'int'
                    elif data_type_raw in ('double', 'real', 'float'):
                        queryformer_datatype = 'float'
                    else:
                        queryformer_datatype = 'misc'
                elif inferred_type == 'integer':
                    queryformer_datatype = 'int'
                elif inferred_type == 'double precision':
                    queryformer_datatype = 'float'
                else:
                    queryformer_datatype = 'misc'
                
                # MIN/MAX/ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆæ•°å€¤å‹ã®å ´åˆï¼‰
                min_val, max_val = None, None
                percentiles = None
                if queryformer_datatype in ('int', 'float'):
                    # MIN/MAXã¯å¿…é ˆï¼ˆQueryFormerã§ä½¿ç”¨ï¼‰
                    min_val, max_val = get_column_min_max(catalog, schema, table, col_name)
                    
                    # ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã¯Trinoç’°å¢ƒã«ã‚ˆã£ã¦ã¯å–å¾—ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚‹
                    # MIN/MAXãŒå–å¾—ã§ããŸå ´åˆã®ã¿ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã‚’å–å¾—
                    if min_val is not None and max_val is not None:
                        percentiles = get_column_percentiles(catalog, schema, table, col_name)
                    else:
                        # MIN/MAXãŒå–å¾—ã§ããªã„å ´åˆã€ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«ã‚‚å–å¾—ã—ãªã„
                        percentiles = None
                
                column_stats[tuple_key] = {
                    'n_distinct': distinct_count if distinct_count is not None else -1,
                    'null_frac': null_frac,
                    'avg_width': (
                        int(data_size / row_count)
                        if data_size is not None and row_count > 0
                        else 0
                    ),
                    'correlation': 0.0,
                    'data_type': inferred_type,
                    'datatype': queryformer_datatype,
                    'min': min_val,
                    'max': max_val,
                    'percentiles': percentiles,
                    'table': table,
                    'column': col_name,
                    'data_size': data_size,
                    'distinct_values_count': distinct_count,
                    'nulls_fraction': null_frac,
                    'low_value': col_stat.get('low_value'),
                    'high_value': col_stat.get('high_value'),
                    'trino_data_type': data_type_raw,
                }
            
            row_count = stats.get('row_count')
            num_columns = len(stats.get('columns', {}))
            print(f"   âœ… è¡Œæ•°: {int(row_count) if row_count else 'N/A'}, "
                  f"ã‚«ãƒ©ãƒ æ•°: {num_columns}")
            
            columns = list(stats.get('columns', {}).items())
            for col_name, col_stat in columns[:5]:
                distinct = col_stat.get('distinct_values_count')
                nulls = col_stat.get('nulls_fraction')
                info_parts = []
                if distinct is not None:
                    info_parts.append(f"distinct={int(distinct)}")
                if nulls is not None:
                    info_parts.append(f"nulls={nulls:.2%}")
                info_str = ", ".join(info_parts) if info_parts else "no stats"
                print(f"     - {col_name}: {info_str}")
            
            if len(columns) > 5:
                print(f"     ... ä»– {len(columns) - 5} ã‚«ãƒ©ãƒ ")
        else:
            print(f"   âŒ çµ±è¨ˆæƒ…å ±ã®å–å¾—ã«å¤±æ•—")
            return False, {}, {}
        
        print()
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã¨JSONå‡ºåŠ›
    metadata = {
        'catalog': catalog,
        'schema': schema,
        'num_tables': len(table_stats),
        'num_columns': len(column_stats)
    }
    
    table_stats_file = schema_output_dir / 'table_stats.json'
    column_stats_file = schema_output_dir / 'column_stats.json'
    metadata_file = schema_output_dir / 'metadata.json'
    
    with open(table_stats_file, 'w', encoding='utf-8') as f:
        json.dump(table_stats, f, indent=2, ensure_ascii=False)
    
    with open(column_stats_file, 'w', encoding='utf-8') as f:
        json.dump(column_stats, f, indent=2, ensure_ascii=False)
    
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ:")
    print(f"   ğŸ“„ {table_stats_file}")
    print(f"   ğŸ“„ {column_stats_file}")
    print(f"   ğŸ“„ {metadata_file}")
    print()
    print(f"ğŸ“ˆ åé›†ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(table_stats)}")
    print(f"ğŸ“ˆ åé›†ã•ã‚ŒãŸã‚«ãƒ©ãƒ æ•°: {len(column_stats)}")
    
    total_rows = sum(stats.get('row_count', 0) or 0 for stats in table_stats.values())
    print(f"ğŸ“Š åˆè¨ˆè¡Œæ•°: {int(total_rows):,}")
    print()
    
    return True, table_stats, column_stats


def main():
    parser = argparse.ArgumentParser(
        description='Trinoã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®çµ±è¨ˆæƒ…å ±ã‚’åé›†'
    )
    parser.add_argument(
        '--catalog',
        default='iceberg',
        help='Trinoã‚«ã‚¿ãƒ­ã‚°åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: icebergï¼‰'
    )
    parser.add_argument(
        '--schema',
        help='ã‚¹ã‚­ãƒ¼ãƒåï¼ˆå˜ä¸€ã‚¹ã‚­ãƒ¼ãƒå‡¦ç†æ™‚ï¼‰'
    )
    parser.add_argument(
        '--schemas',
        help='ã‚¹ã‚­ãƒ¼ãƒåã®ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šãƒªã‚¹ãƒˆï¼ˆè¤‡æ•°ã‚¹ã‚­ãƒ¼ãƒå‡¦ç†æ™‚ï¼‰'
    )
    parser.add_argument(
        '--all-schemas',
        action='store_true',
        help='å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚¹ã‚­ãƒ¼ãƒï¼‰ã®çµ±è¨ˆæƒ…å ±ã‚’åé›†'
    )
    parser.add_argument(
        '--tables',
        help='ãƒ†ãƒ¼ãƒ–ãƒ«åã®ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šãƒªã‚¹ãƒˆï¼ˆçœç•¥æ™‚ã¯å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        default='datasets_statistics',
        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: datasets_statisticsï¼‰'
    )
    parser.add_argument(
        '--continue-on-error',
        action='store_true',
        help='ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶šè¡Œã™ã‚‹ï¼ˆè¤‡æ•°ã‚¹ã‚­ãƒ¼ãƒå‡¦ç†æ™‚ï¼‰'
    )
    
    args = parser.parse_args()
    
    # ã‚¹ã‚­ãƒ¼ãƒãƒªã‚¹ãƒˆã‚’æ±ºå®š
    if args.all_schemas:
        schemas = AVAILABLE_DATASETS
    elif args.schemas:
        schemas = [s.strip() for s in args.schemas.split(',')]
    elif args.schema:
        schemas = [args.schema]
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: imdb
        schemas = ['imdb']
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_dir_base = Path(args.output_dir)
    output_dir_base.mkdir(parents=True, exist_ok=True)
    
    # è¤‡æ•°ã‚¹ã‚­ãƒ¼ãƒå‡¦ç†ã‹ã©ã†ã‹
    is_multi_schema = len(schemas) > 1
    
    if is_multi_schema:
        print(f"ğŸš€ Trinoçµ±è¨ˆæƒ…å ±ä¸€æ‹¬åé›†é–‹å§‹")
        print(f"  Catalog: {args.catalog}")
        print(f"  ã‚¹ã‚­ãƒ¼ãƒæ•°: {len(schemas)}")
        print(f"  å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir_base}")
        print()
        
        success_count = 0
        failed_schemas = []
        
        for i, schema in enumerate(schemas, 1):
            print(f"\n{'='*80}")
            print(f"[{i}/{len(schemas)}] {schema} ã®çµ±è¨ˆæƒ…å ±ã‚’åé›†ä¸­...")
            print(f"{'='*80}\n")
            
            tables = None
            if args.tables:
                tables = [t.strip() for t in args.tables.split(',')]
            
            success, _, _ = collect_schema_stats(
                catalog=args.catalog,
                schema=schema,
                tables=tables,
                output_dir_base=output_dir_base
            )
            
            if success:
                success_count += 1
                print(f"âœ… {schema} ã®çµ±è¨ˆæƒ…å ±åé›†å®Œäº†\n")
            else:
                failed_schemas.append(schema)
                print(f"âŒ {schema} ã®çµ±è¨ˆæƒ…å ±åé›†å¤±æ•—\n")
                if not args.continue_on_error:
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šå‡¦ç†ã‚’ä¸­æ–­ã—ã¾ã™")
                    break
        
        # æœ€çµ‚ã‚µãƒãƒªãƒ¼
        print("\n" + "=" * 80)
        print("ğŸ“Š çµ±è¨ˆæƒ…å ±åé›†å®Œäº†")
        print("=" * 80)
        print(f"âœ… æˆåŠŸ: {success_count}/{len(schemas)} ã‚¹ã‚­ãƒ¼ãƒ")
        
        if failed_schemas:
            print(f"âŒ å¤±æ•—: {len(failed_schemas)} ã‚¹ã‚­ãƒ¼ãƒ")
            print(f"   å¤±æ•—ã—ãŸã‚¹ã‚­ãƒ¼ãƒ: {', '.join(failed_schemas)}")
        else:
            print("ğŸ‰ ã™ã¹ã¦ã®ã‚¹ã‚­ãƒ¼ãƒã®çµ±è¨ˆæƒ…å ±åé›†ã«æˆåŠŸã—ã¾ã—ãŸï¼")
        
        print()
        print(f"ğŸ“ çµ±è¨ˆæƒ…å ±ã®ä¿å­˜å…ˆ: {output_dir_base}/")
        
        # ç”Ÿæˆã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒªã‚¹ãƒˆ
        if output_dir_base.exists():
            subdirs = [d.name for d in output_dir_base.iterdir() if d.is_dir()]
            print(f"   ç”Ÿæˆã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ•°: {len(subdirs)}")
            for subdir in sorted(subdirs)[:10]:  # æœ€åˆã®10å€‹ã®ã¿è¡¨ç¤º
                print(f"   - {subdir}")
            if len(subdirs) > 10:
                print(f"   ... ä»– {len(subdirs) - 10} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
        
        sys.exit(0 if success_count == len(schemas) else 1)
    
    else:
        # å˜ä¸€ã‚¹ã‚­ãƒ¼ãƒå‡¦ç†
        schema = schemas[0]
        print(f"ğŸ“Š Trinoçµ±è¨ˆæƒ…å ±åé›†é–‹å§‹")
        print(f"  Catalog: {args.catalog}")
        print(f"  Schema: {schema}")
        print()
        
        tables = None
        if args.tables:
            tables = [t.strip() for t in args.tables.split(',')]
        
        success, _, _ = collect_schema_stats(
            catalog=args.catalog,
            schema=schema,
            tables=tables,
            output_dir_base=output_dir_base
        )
        
        if success:
            print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
            schema_dir_name = f"{args.catalog}_{schema}"
            schema_output_dir = output_dir_base / schema_dir_name
            print(f"   çµ±è¨ˆæƒ…å ±ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {schema_output_dir}")
            print(f"   ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ™‚ã«ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦ãã ã•ã„")
            sys.exit(0)
        else:
            print(f"\nâŒ çµ±è¨ˆæƒ…å ±ã®åé›†ã«å¤±æ•—ã—ã¾ã—ãŸ")
            sys.exit(1)


if __name__ == '__main__':
    main()