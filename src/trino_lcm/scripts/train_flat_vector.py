"""
Trino Flat-Vector Model Training Script

Trinoã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³å‘ã‘ã®Flat-Vectorãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚
ã“ã‚Œã¯ã€æ—¢å­˜ã®PostgreSQLç”¨Flat-Vectorãƒ¢ãƒ‡ãƒ«ã‚’trinoå‘ã‘ã«å†å®Ÿè£…ã—ãŸã‚‚ã®ã§ã™ã€‚

Usage:
    # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å®Ÿè¡Œ
    python -m trino_lcm.scripts.train_flat_vector \
        --train_files accidents_valid_verbose.txt \
        --test_file accidents_valid_verbose.txt \
        --output_dir models/trino_flat_vector \
        --epochs 1000
"""

import sys
import os
import warnings
from pathlib import Path
import argparse
import json
from typing import Optional, Sequence
import numpy as np
import torch

# Suppress torchdata deprecation warnings
warnings.filterwarnings('ignore', category=UserWarning, module='torchdata')

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®šï¼ˆå¿…é ˆ - importå‰ã«å®Ÿè¡Œï¼‰
for i in range(11):
    env_key = f'NODE{i:02d}'
    env_value = os.environ.get(env_key)
    if env_value in (None, '', 'None'):
        os.environ[env_key] = '[]'

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒsrc/trino_lcm/scripts/ã«ã‚ã‚‹å ´åˆã€src/ã‚’è¦ªãƒ‘ã‚¹ã«è¿½åŠ 
script_dir = Path(__file__).resolve().parent
src_dir = script_dir.parent.parent
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

from trino_lcm.models.flat_vector import (
    load_trino_plans_from_files,
    collect_operator_types,
    create_flat_vector_dataset,
    train_flat_vector_model,
    predict_flat_vector_model
)
from training.training.metrics import QError, RMSE, MAPE


def evaluate_with_metrics(bst, X, y, dataset_name="Test"):
    """
    æ—¢å­˜ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡
    
    Args:
        bst: LightGBMãƒ¢ãƒ‡ãƒ«
        X: ç‰¹å¾´é‡
        y: ãƒ©ãƒ™ãƒ«
        dataset_name: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå
    
    Returns:
        è©•ä¾¡ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¾žæ›¸
    """
    # äºˆæ¸¬
    y_pred = predict_flat_vector_model(bst, X)
    
    # Q-Errorè¨ˆç®—ã®ãŸã‚ã€min_valã‚’è¨­å®šï¼ˆ0é™¤ç®—ã‚’é˜²ãï¼‰
    # ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ã®å®Ÿè¡Œæ™‚é–“ã¯100msï¼ˆ0.1ç§’ï¼‰ï½ž30ç§’ã®ç¯„å›²
    min_val = 0.1  # 0.1ç§’ = 100ãƒŸãƒªç§’
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®šç¾©
    metrics = [
        RMSE(),
        MAPE(),
        QError(percentile=50, min_val=min_val, early_stopping_metric=True),
        QError(percentile=95, min_val=min_val),
        QError(percentile=99, min_val=min_val),
        QError(percentile=100, min_val=min_val)
    ]
    
    # è©•ä¾¡å®Ÿè¡Œï¼ˆQErrorã‚¯ãƒ©ã‚¹å†…ã§ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ãŒè¡Œã‚ã‚Œã‚‹ï¼‰
    metrics_dict = {}
    for metric in metrics:
        metric.evaluate(
            metrics_dict=metrics_dict,
            model=None,
            labels=y,
            preds=y_pred,
            probs=None
        )
    
    # çµæžœã‚’æ•´å½¢
    results = {
        'dataset': dataset_name,
        'num_samples': len(y),
        'rmse': metrics_dict.get('val_mse', 0.0),
        'mape': metrics_dict.get('val_mape', 0.0),
        'median_q_error': metrics_dict.get('val_median_q_error_50', 0.0),
        'p95_q_error': metrics_dict.get('val_median_q_error_95', 0.0),
        'p99_q_error': metrics_dict.get('val_median_q_error_99', 0.0),
        'max_q_error': metrics_dict.get('val_median_q_error_100', 0.0)
    }
    
    print(f"\nðŸ“Š ã€{dataset_name}ã‚»ãƒƒãƒˆè©•ä¾¡çµæžœã€‘")
    print(f"  - ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(y)}")
    print(f"  - RMSE: {results['rmse']:.4f}ç§’ ({results['rmse']*1000:.2f}ms)")
    print(f"  - MAPE: {results['mape']:.4f}")
    print(f"  - Median Q-Error: {results['median_q_error']:.4f}")
    print(f"  - P95 Q-Error: {results['p95_q_error']:.4f}")
    print(f"  - P99 Q-Error: {results['p99_q_error']:.4f}")
    print(f"  - Max Q-Error: {results['max_q_error']:.4f}")
    
    return results


def build_parser() -> argparse.ArgumentParser:
    """Build the argument parser for Flat-Vector training."""
    parser = argparse.ArgumentParser(
        description='Train Trino Flat-Vector Model (Trinoå‘ã‘å†å®Ÿè£…ç‰ˆ)'
    )
    
    # ãƒ¢ãƒ¼ãƒ‰é¸æŠž
    parser.add_argument(
        '--mode',
        type=str,
        choices=['train', 'train_multi_all'],
        default='train',
        help='Training mode: train (single dataset) or train_multi_all (leave-one-out across all datasets)'
    )
    
    # ãƒ‡ãƒ¼ã‚¿é–¢é€£ã®å¼•æ•°
    parser.add_argument('--train_files', type=str, required=False,
                        help='ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚«ãƒ³ãƒžåŒºåˆ‡ã‚Šã€trainãƒ¢ãƒ¼ãƒ‰ã§å¿…é ˆï¼‰')
    parser.add_argument('--test_file', type=str, required=False,
                        help='ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆtrainãƒ¢ãƒ¼ãƒ‰ã§å¿…é ˆï¼‰')
    
    # ãƒ¢ãƒ‡ãƒ«é–¢é€£ã®å¼•æ•°
    parser.add_argument('--output_dir', type=str, default='models/trino_flat_vector',
                        help='ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--num_boost_round', type=int, default=1000,
                        help='ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰æ•°')
    parser.add_argument('--early_stopping_rounds', type=int, default=20,
                        help='æ—©æœŸåœæ­¢ãƒ©ã‚¦ãƒ³ãƒ‰æ•°')
    parser.add_argument('--seed', type=int, default=42,
                        help='ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰')
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢é€£ã®å¼•æ•°
    parser.add_argument('--max_plans_per_file', type=int, default=None,
                        help='å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€æœ€å¤§ãƒ—ãƒ©ãƒ³æ•°')
    parser.add_argument('--val_ratio', type=float, default=0.15,
                        help='æ¤œè¨¼ã‚»ãƒƒãƒˆã®å‰²åˆ')
    parser.add_argument('--use_act_card', action='store_true',
                        help='å®Ÿéš›ã®ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã‚’ä½¿ç”¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æŽ¨å®šã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ï¼‰')
    parser.add_argument(
        '--plans_dir',
        type=str,
        default='/Users/an/query_engine/explain_analyze_results/',
        help='Directory containing .txt plan files for multiple datasets (required for train_multi_all mode)'
    )
    
    return parser


def run(args) -> int:
    """Run Flat-Vector training with parsed arguments."""
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã®è¨­å®š
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 80)
    print("Trino Flat-Vector Model Training")
    print("ï¼ˆPostgreSQLç”¨Flat-Vectorãƒ¢ãƒ‡ãƒ«ã®trinoå‘ã‘å†å®Ÿè£…ï¼‰")
    print(f"Mode: {args.mode}")
    print("=" * 80)
    
    # train_multi_allãƒ¢ãƒ¼ãƒ‰ã®å‡¦ç†
    if args.mode == 'train_multi_all':
        return run_train_multi_all(args, output_dir)
    
    # å¾“æ¥ã®trainãƒ¢ãƒ¼ãƒ‰
    if not args.train_files or not args.test_file:
        raise ValueError("--train_files and --test_file are required for train mode")
    
    print(f"Train files: {args.train_files}")
    print(f"Test file: {args.test_file}")
    print(f"Output directory: {args.output_dir}")
    print(f"Use actual cardinality: {args.use_act_card}")
    print()
    
    # 1. ãƒ—ãƒ©ãƒ³ã®èª­ã¿è¾¼ã¿
    print("ðŸ“‚ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ—ãƒ©ãƒ³ã®èª­ã¿è¾¼ã¿")
    train_file_paths = [Path(p.strip()) for p in args.train_files.split(',')]
    test_file_path = Path(args.test_file)
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ©ãƒ³ã®èª­ã¿è¾¼ã¿
    train_plans = load_trino_plans_from_files(train_file_paths, args.max_plans_per_file)
    
    # ãƒ†ã‚¹ãƒˆãƒ—ãƒ©ãƒ³ã®èª­ã¿è¾¼ã¿
    test_plans = load_trino_plans_from_files([test_file_path], args.max_plans_per_file)
    
    print()
    
    # 2. æ¼”ç®—å­ã‚¿ã‚¤ãƒ—ã®åŽé›†
    print("ðŸ“Š ã‚¹ãƒ†ãƒƒãƒ—2: æ¼”ç®—å­ã‚¿ã‚¤ãƒ—ã®åŽé›†")
    op_idx_dict = collect_operator_types(train_plans)
    print()
    
    # 3. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/æ¤œè¨¼ã‚»ãƒƒãƒˆã®åˆ†å‰²
    print("ðŸ“Š ã‚¹ãƒ†ãƒƒãƒ—3: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/æ¤œè¨¼ã‚»ãƒƒãƒˆã®åˆ†å‰²")
    val_size = int(len(train_plans) * args.val_ratio)
    train_size = len(train_plans) - val_size
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ£ãƒƒãƒ•ãƒ«
    indices = list(range(len(train_plans)))
    np.random.shuffle(indices)
    
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]
    
    train_plans_split = [train_plans[i] for i in train_indices]
    val_plans_split = [train_plans[i] for i in val_indices]
    
    print(f"  - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ©ãƒ³: {len(train_plans_split)}")
    print(f"  - æ¤œè¨¼ãƒ—ãƒ©ãƒ³: {len(val_plans_split)}")
    print(f"  - ãƒ†ã‚¹ãƒˆãƒ—ãƒ©ãƒ³: {len(test_plans)}")
    print()
    
    # 4. ç‰¹å¾´é‡ã®æŠ½å‡º
    print("ðŸ”§ ã‚¹ãƒ†ãƒƒãƒ—4: ç‰¹å¾´é‡ã®æŠ½å‡º")
    
    print("  - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆ...")
    X_train, y_train = create_flat_vector_dataset(train_plans_split, op_idx_dict, args.use_act_card)
    
    print("  - æ¤œè¨¼ã‚»ãƒƒãƒˆ...")
    X_val, y_val = create_flat_vector_dataset(val_plans_split, op_idx_dict, args.use_act_card, verbose=False)
    
    print("  - ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ...")
    X_test, y_test = create_flat_vector_dataset(test_plans, op_idx_dict, args.use_act_card, verbose=False)
    
    print(f"\n  - ç‰¹å¾´é‡æ¬¡å…ƒæ•°: {X_train.shape[1]}")
    print(f"  - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«: {len(X_train)}")
    print(f"  - æ¤œè¨¼ã‚µãƒ³ãƒ—ãƒ«: {len(X_val)}")
    print(f"  - ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«: {len(X_test)}")
    print()
    
    # 5. ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
    print("ðŸš€ ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°")
    bst = train_flat_vector_model(
        X_train, y_train,
        X_val, y_val,
        num_boost_round=args.num_boost_round,
        early_stopping_rounds=args.early_stopping_rounds,
        seed=args.seed,
        verbose=True
    )
    
    # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
    model_path = output_dir / f'flat_vector_model_{args.seed}.txt'
    bst.save_model(str(model_path))
    print(f"âœ… ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: {model_path}")
    print()
    
    # 6. ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
    print("ðŸ“Š ã‚¹ãƒ†ãƒƒãƒ—6: ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡")
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
    train_metrics = evaluate_with_metrics(bst, X_train, y_train, "Train")
    
    # æ¤œè¨¼ã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
    val_metrics = evaluate_with_metrics(bst, X_val, y_val, "Validation")
    
    # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
    test_metrics = evaluate_with_metrics(bst, X_test, y_test, "Test")
    
    # 7. çµæžœã®ä¿å­˜
    print("\nðŸ’¾ ã‚¹ãƒ†ãƒƒãƒ—7: çµæžœã®ä¿å­˜")
    
    # æ¼”ç®—å­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¾žæ›¸ã®ä¿å­˜
    op_idx_path = output_dir / f'op_idx_dict_{args.seed}.json'
    with open(op_idx_path, 'w') as f:
        json.dump(op_idx_dict, f, indent=2)
    print(f"  - æ¼”ç®—å­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¾žæ›¸: {op_idx_path}")
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¿å­˜
    metrics_path = output_dir / f'metrics_{args.seed}.json'
    with open(metrics_path, 'w') as f:
        json.dump({
            'train': train_metrics,
            'validation': val_metrics,
            'test': test_metrics,
            'hyperparameters': {
                'num_boost_round': args.num_boost_round,
                'early_stopping_rounds': args.early_stopping_rounds,
                'val_ratio': args.val_ratio,
                'use_act_card': args.use_act_card,
                'seed': args.seed
            }
        }, f, indent=2)
    print(f"  - ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {metrics_path}")
    
    print()
    print("=" * 80)
    print("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")
    print(f"Validation Median Q-Error: {val_metrics['median_q_error']:.4f}")
    print(f"Test Median Q-Error: {test_metrics['median_q_error']:.4f}")
    print(f"Model saved to: {model_path}")
    print("=" * 80)
    
    return 0


def run_train_multi_all(args, output_dir: Path) -> int:
    """20å€‹ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦leave-one-out validationã‚’å®Ÿè¡Œ"""
    # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹20å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ï¼‰
    ALL_DATASETS = [
        'accidents', 'airline', 'baseball', 'basketball', 'carcinogenesis',
        'consumer', 'credit', 'employee', 'fhnk', 'financial', 'geneea',
        'genome', 'hepatitis', 'imdb', 'movielens', 'seznam', 'ssb',
        'tournament', 'tpc_h', 'walmart'
    ]
    
    plans_dir = Path(args.plans_dir)
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç¢ºèª
    txt_files = sorted([p for p in plans_dir.glob('*.txt')])
    available_datasets = set()
    for p in txt_files:
        stem = p.stem  # .txtã‚’é™¤ã„ãŸãƒ•ã‚¡ã‚¤ãƒ«å
        parts = stem.split('_')
        # æœ€é•·ãƒžãƒƒãƒ: ALL_DATASETSã‹ã‚‰æœ€é•·ã®ä¸€è‡´ã‚’æŽ¢ã™ï¼ˆtpc_hãªã©ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã«å¯¾å¿œï¼‰
        matched_dataset = None
        for i in range(len(parts), 0, -1):
            candidate = '_'.join(parts[:i])
            if candidate in ALL_DATASETS:
                matched_dataset = candidate
                break
        if matched_dataset:
            available_datasets.add(matched_dataset)
    
    available_datasets = sorted(list(available_datasets))
    print(f"\n{'='*80}")
    print(f"Leave-One-Out Validation for All Datasets (Flat-Vector)")
    print(f"{'='*80}")
    print(f"åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(available_datasets)} / {len(ALL_DATASETS)}")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {', '.join(available_datasets)}")
    print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print(f"{'='*80}\n")
    
    # æœ€åˆã«1å›žã ã‘å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ—ãƒ©ãƒ³ã‚’èª­ã¿è¾¼ã‚€
    def load_all_datasets_once_flat_vector(plans_dir: Path, available_datasets: list, max_plans_per_file=None):
        """å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ—ãƒ©ãƒ³ã‚’1å›žã ã‘èª­ã¿è¾¼ã‚€"""
        def infer_dataset_name(p: Path, ALL_DATASETS: list) -> str:
            stem = p.stem
            parts = stem.split('_')
            matched_dataset = None
            for i in range(len(parts), 0, -1):
                candidate = '_'.join(parts[:i])
                if candidate in ALL_DATASETS:
                    matched_dataset = candidate
                    break
            if matched_dataset:
                return matched_dataset
            return stem.split('_')[0]
        
        txt_files = sorted([p for p in plans_dir.glob('*.txt')])
        dataset_to_files = {}
        for p in txt_files:
            ds = infer_dataset_name(p, ALL_DATASETS)
            if ds in available_datasets:
                dataset_to_files.setdefault(ds, []).append(p)
        
        all_plans_by_dataset = {}
        print("=" * 80)
        print("ã‚¹ãƒ†ãƒƒãƒ—0: å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ—ãƒ©ãƒ³ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        print("=" * 80)
        print()
        
        for ds in available_datasets:
            if ds in dataset_to_files:
                files = dataset_to_files[ds]
                print(f"  èª­ã¿è¾¼ã¿ä¸­: {ds} ({len(files)} ãƒ•ã‚¡ã‚¤ãƒ«)...")
                plans = load_trino_plans_from_files(files, max_plans_per_file)
                all_plans_by_dataset[ds] = plans
                print(f"    âœ… {ds}: {len(plans)} ãƒ—ãƒ©ãƒ³")
        
        print(f"\nâœ… å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿å®Œäº†")
        print(f"  - èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(all_plans_by_dataset)}")
        for ds, plans in all_plans_by_dataset.items():
            print(f"    - {ds}: {len(plans)} ãƒ—ãƒ©ãƒ³")
        print()
        
        return all_plans_by_dataset
    
    all_plans_by_dataset = load_all_datasets_once_flat_vector(
        plans_dir=plans_dir,
        available_datasets=available_datasets,
        max_plans_per_file=args.max_plans_per_file
    )
    
    # å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰æ¼”ç®—å­ã‚¿ã‚¤ãƒ—ã‚’äº‹å‰ã«åŽé›†ï¼ˆæœªçŸ¥ã®æ¼”ç®—å­ã‚¿ã‚¤ãƒ—ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
    print(f"\n{'='*80}")
    print("ðŸ“Š å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰æ¼”ç®—å­ã‚¿ã‚¤ãƒ—ã‚’åŽé›†ä¸­...")
    print(f"{'='*80}")
    all_plans = []
    for plans in all_plans_by_dataset.values():
        all_plans.extend(plans)
    global_op_idx_dict = collect_operator_types(all_plans)
    print()
    
    # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    results_summary = []
    
    for idx, test_dataset in enumerate(available_datasets, 1):
        print(f"\n{'#'*80}")
        print(f"# [{idx}/{len(available_datasets)}] Testing dataset: {test_dataset}")
        print(f"{'#'*80}\n")
        
        try:
            # æ—¢ã«èª­ã¿è¾¼ã‚“ã ãƒ—ãƒ©ãƒ³ã‹ã‚‰train/testã‚’åˆ†å‰²
            if test_dataset not in all_plans_by_dataset:
                print(f"âš ï¸  {test_dataset}: ãƒ—ãƒ©ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                results_summary.append({
                    'test_dataset': test_dataset,
                    'status': 'skipped',
                    'reason': 'missing plans'
                })
                continue
            
            train_plans = []
            test_plans = all_plans_by_dataset[test_dataset]
            
            for ds, plans in all_plans_by_dataset.items():
                if ds != test_dataset:
                    train_plans.extend(plans)
            
            if not train_plans or not test_plans:
                print(f"âš ï¸  {test_dataset}: è¨“ç·´ãƒ—ãƒ©ãƒ³ã¾ãŸã¯ãƒ†ã‚¹ãƒˆãƒ—ãƒ©ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                results_summary.append({
                    'test_dataset': test_dataset,
                    'status': 'skipped',
                    'reason': 'missing plans'
                })
                continue
            
            # ãƒ¢ãƒ‡ãƒ«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            model_dir = output_dir / f'models_{test_dataset}'
            model_dir.mkdir(parents=True, exist_ok=True)
            
            print(f"ðŸ“Š Leave-One-Out Validation [{idx}/{len(available_datasets)}]:")
            print(f"  - Training datasets: {len(all_plans_by_dataset) - 1} datasets")
            print(f"  - Training plans: {len(train_plans)}")
            print(f"  - Test dataset: {test_dataset}")
            print(f"  - Test plans: {len(test_plans)}")
            print()
            
            # æ¼”ç®—å­ã‚¿ã‚¤ãƒ—ã®è¾žæ›¸ã¯å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰äº‹å‰ã«åŽé›†ã—ãŸã‚‚ã®ã‚’ä½¿ç”¨
            op_idx_dict = global_op_idx_dict
            
            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/æ¤œè¨¼ã‚»ãƒƒãƒˆã®åˆ†å‰²ï¼ˆ19å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’train/valã«åˆ†å‰²ï¼‰
            val_size = int(len(train_plans) * args.val_ratio)
            train_size = len(train_plans) - val_size
            
            indices = list(range(len(train_plans)))
            np.random.shuffle(indices)
            
            train_indices = indices[:train_size]
            val_indices = indices[train_size:]
            
            train_plans_split = [train_plans[i] for i in train_indices]
            val_plans_split = [train_plans[i] for i in val_indices]
            
            print(f"âœ… 19å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ä½œæˆ:")
            print(f"  - Train plans: {len(train_plans_split)}")
            print(f"  - Val plans (from 19 datasets): {len(val_plans_split)}")
            print()
            
            # ç‰¹å¾´é‡ã®æŠ½å‡º
            print("ðŸ”§ ç‰¹å¾´é‡ã®æŠ½å‡º...")
            X_train, y_train = create_flat_vector_dataset(train_plans_split, op_idx_dict, args.use_act_card, verbose=False)
            X_val, y_val = create_flat_vector_dataset(val_plans_split, op_idx_dict, args.use_act_card, verbose=False)
            X_test, y_test = create_flat_vector_dataset(test_plans, op_idx_dict, args.use_act_card, verbose=False)
            
            print(f"  - ç‰¹å¾´é‡æ¬¡å…ƒæ•°: {X_train.shape[1]}")
            print(f"  - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
            print()
            
            # ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
            print("ðŸš€ ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°...")
            bst = train_flat_vector_model(
                X_train, y_train,
                X_val, y_val,
                num_boost_round=args.num_boost_round,
                early_stopping_rounds=args.early_stopping_rounds,
                seed=args.seed,
                verbose=False
            )
            
            # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
            model_path = model_dir / f'flat_vector_model_{args.seed}.txt'
            bst.save_model(str(model_path))
            
            # æ¼”ç®—å­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¾žæ›¸ã®ä¿å­˜
            op_idx_path = model_dir / f'op_idx_dict_{args.seed}.json'
            with open(op_idx_path, 'w') as f:
                json.dump(op_idx_dict, f, indent=2)
            
            # ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡
            val_metrics = evaluate_with_metrics(bst, X_val, y_val, "Validation")
            test_metrics = evaluate_with_metrics(bst, X_test, y_test, "Test")
            
            # ãƒ†ã‚¹ãƒˆçµæžœã‚’ä¿å­˜
            test_results = {
                'test_median_q_error': float(test_metrics['median_q_error']),
                'test_mean_q_error': float(np.mean([
                    test_metrics.get('p95_q_error', 0),
                    test_metrics.get('p99_q_error', 0),
                    test_metrics.get('max_q_error', 0)
                ])) if any(k in test_metrics for k in ['p95_q_error', 'p99_q_error', 'max_q_error']) else None,
                'test_rmse': float(test_metrics.get('rmse', 0)),
                'test_samples': len(test_plans)
            }
            
            results_file = model_dir / 'test_results.json'
            with open(results_file, 'w') as f:
                json.dump(test_results, f, indent=2)
            
            results_summary.append({
                'test_dataset': test_dataset,
                'model_dir': str(model_dir),
                'val_median_q_error': float(val_metrics['median_q_error']),
                **test_results,
                'status': 'completed'
            })
            
            print(f"âœ… [{idx}/{len(available_datasets)}] {test_dataset} ã®è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆå®Œäº†")
            print(f"   Validation Median Q-Error: {val_metrics['median_q_error']:.4f}")
            print(f"   Test Median Q-Error: {test_metrics['median_q_error']:.4f}")
            print(f"   ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {model_dir}")
            print()
            
        except Exception as e:
            print(f"âŒ [{idx}/{len(available_datasets)}] {test_dataset} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ:")
            print(f"   {e}")
            import traceback
            traceback.print_exc()
            results_summary.append({
                'test_dataset': test_dataset,
                'status': 'failed',
                'error': str(e)
            })
            continue
    
    # å…¨ä½“ã®ã‚µãƒžãƒªãƒ¼ã‚’ä¿å­˜
    summary_file = output_dir / 'leave_one_out_summary.json'
    with open(summary_file, 'w') as f:
        json.dump({
            'total_datasets': len(available_datasets),
            'completed': len([r for r in results_summary if r['status'] == 'completed']),
            'failed': len([r for r in results_summary if r['status'] == 'failed']),
            'skipped': len([r for r in results_summary if r.get('status') == 'skipped']),
            'results': results_summary
        }, f, indent=2)
    
    print("\n" + "=" * 80)
    print("ðŸŽ‰ å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®Leave-One-Out Validationå®Œäº†ï¼")
    print("=" * 80)
    print(f"å®Œäº†: {len([r for r in results_summary if r['status'] == 'completed'])}/{len(available_datasets)}")
    print(f"å¤±æ•—: {len([r for r in results_summary if r['status'] == 'failed'])}/{len(available_datasets)}")
    print(f"ã‚µãƒžãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«: {summary_file}")
    print()
    
    return 0


def main(argv: Optional[Sequence[str]] = None) -> int:
    """Main entry point for Flat-Vector training."""
    parser = build_parser()
    args = parser.parse_args(argv)
    return run(args)


if __name__ == "__main__":
    from typing import Optional, Sequence
    import sys
    sys.exit(main())

