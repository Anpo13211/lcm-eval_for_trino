"""
Trino DACE Model Training Script

Trinoã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³å‘ã‘ã®DACEãƒ¢ãƒ‡ãƒ«ï¼ˆTransformer-basedï¼‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚

Usage:
    # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å®Ÿè¡Œ
    python -m trino_lcm.scripts.train_dace \
        --workload_runs path/to/workload.json \
        --statistics_file path/to/feature_statistics.json \
        --output_dir models/trino_dace \
        --batch_size 32 \
        --epochs 100
"""

import sys
import os
import warnings

# Suppress torchdata deprecation warnings
warnings.filterwarnings('ignore', category=UserWarning, module='torchdata')

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®šï¼ˆå¿…é ˆ - importå‰ã«å®Ÿè¡Œï¼‰
for i in range(11):
    env_key = f'NODE{i:02d}'
    env_value = os.environ.get(env_key)
    if env_value in (None, '', 'None'):
        os.environ[env_key] = '[]'

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒsrc/trino_lcm/scripts/ã«ã‚ã‚‹å ´åˆã€src/ã‚’è¦ªãƒ‘ã‚¹ã«è¿½åŠ 
from pathlib import Path
script_dir = Path(__file__).resolve().parent
src_dir = script_dir.parent.parent
if str(src_dir) not in sys.path:
    sys.path.insert(0, str(src_dir))

import argparse
import json
from typing import Optional, Sequence
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm

from models.dace.dace_dataset_trino import create_dace_dataloader
from models.dace.dace_model import DACELora
from classes.classes import DACEModelConfig, DataLoaderOptions
from classes.workload_runs import WorkloadRuns
from training.training.metrics import QError, RMSE
from training.featurizations import DACEFeaturization
from training.preprocessing.feature_statistics import FeatureType
from trino_lcm.scripts.train_zeroshot import load_plans_from_files
from sklearn.preprocessing import RobustScaler
import collections


def build_parser() -> argparse.ArgumentParser:
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’æ§‹ç¯‰"""
    parser = argparse.ArgumentParser(
        description="Train DACE model for Trino query runtime prediction"
    )
    
    # ãƒ¢ãƒ¼ãƒ‰é¸æŠ
    parser.add_argument(
        '--mode',
        type=str,
        choices=['train', 'train_multi_all'],
        default='train',
        help='Training mode: train (single dataset) or train_multi_all (leave-one-out across all datasets)'
    )
    
    # ãƒ‡ãƒ¼ã‚¿é–¢é€£
    parser.add_argument(
        '--workload_runs',
        type=str,
        nargs='+',
        required=False,
        help='Paths to workload run files (JSON or Trino .txt files) for training (required for train mode)'
    )
    parser.add_argument(
        '--test_workload_runs',
        type=str,
        nargs='+',
        default=None,
        help='Paths to workload run files (JSON or Trino .txt files) for testing'
    )
    parser.add_argument(
        '--statistics_file',
        type=str,
        default=None,
        help='Path to feature statistics JSON file (optional, will be auto-generated if not provided)'
    )
    parser.add_argument(
        '--train_files',
        type=str,
        nargs='+',
        default=None,
        help='Paths to Trino EXPLAIN ANALYZE .txt files for training (used to generate statistics if --statistics_file not provided)'
    )
    parser.add_argument(
        '--max_plans_per_file',
        type=int,
        default=None,
        help='Maximum number of plans to parse per file (for statistics generation)'
    )
    parser.add_argument(
        '--val_ratio',
        type=float,
        default=0.15,
        help='Validation split ratio (default: 0.15)'
    )
    parser.add_argument(
        '--plans_dir',
        type=str,
        default='/Users/an/query_engine/explain_analyze_results/',
        help='Directory containing .txt plan files for multiple datasets (required for train_multi_all mode)'
    )
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    parser.add_argument(
        '--batch_size',
        type=int,
        default=32,
        help='Batch size for training (default: 32)'
    )
    parser.add_argument(
        '--hidden_dim',
        type=int,
        default=128,
        help='Hidden dimension for transformer (default: 128)'
    )
    parser.add_argument(
        '--node_length',
        type=int,
        default=18,
        help='Length of node feature vector (default: 18)'
    )
    parser.add_argument(
        '--pad_length',
        type=int,
        default=50,
        help='Maximum number of nodes (padding length) (default: 50)'
    )
    parser.add_argument(
        '--max_runtime',
        type=float,
        default=30000.0,
        help='Maximum runtime for normalization (ms) (default: 30000)'
    )
    parser.add_argument(
        '--loss_weight',
        type=float,
        default=0.5,
        help='Loss weight for height-based weighting (not used in Trino mode) (default: 0.5)'
    )
    
    # è¨“ç·´è¨­å®š
    parser.add_argument(
        '--epochs',
        type=int,
        default=100,
        help='Number of training epochs (default: 100)'
    )
    parser.add_argument(
        '--learning_rate',
        type=float,
        default=1e-3,
        help='Learning rate (default: 1e-3)'
    )
    parser.add_argument(
        '--num_workers',
        type=int,
        default=4,
        help='Number of dataloader workers (default: 4)'
    )
    parser.add_argument(
        '--device',
        type=str,
        default='cuda' if torch.cuda.is_available() else 'cpu',
        help='Device to use for training (default: cuda if available)'
    )
    parser.add_argument(
        '--cap_training_samples',
        type=int,
        default=None,
        help='Cap number of training samples (default: None)'
    )
    
    # å‡ºåŠ›è¨­å®š
    parser.add_argument(
        '--output_dir',
        type=str,
        required=True,
        help='Directory to save model checkpoints'
    )
    parser.add_argument(
        '--save_every',
        type=int,
        default=10,
        help='Save checkpoint every N epochs (default: 10)'
    )
    parser.add_argument(
        '--log_every',
        type=int,
        default=1,
        help='Log metrics every N epochs (default: 1)'
    )
    
    return parser


def train_epoch(model, train_loader, optimizer, device, epoch):
    """1ã‚¨ãƒãƒƒã‚¯ã®è¨“ç·´"""
    model.train()
    total_loss = 0.0
    total_samples = 0
    
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}")
    for batch in pbar:
        seq_encodings, attention_masks, loss_masks, run_times, labels, sample_idxs = batch
        
        # ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•
        seq_encodings = seq_encodings.to(device)
        attention_masks = attention_masks.to(device)
        loss_masks = loss_masks.to(device)
        run_times = run_times.to(device)
        labels = labels.to(device)
        
        # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
        predictions = model((seq_encodings, attention_masks, loss_masks, run_times))
        
        # æå¤±è¨ˆç®—ï¼ˆDaceLossãŒè‡ªå‹•çš„ã«ä½¿ç”¨ã•ã‚Œã‚‹ï¼‰
        loss = model.loss_fxn(predictions, labels)
        
        # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item() * len(sample_idxs)
        total_samples += len(sample_idxs)
        
        pbar.set_postfix({'loss': loss.item()})
    
    avg_loss = total_loss / total_samples if total_samples > 0 else 0
    return avg_loss


def validate(model, val_loader, device):
    """æ¤œè¨¼"""
    model.eval()
    all_predictions = []
    all_labels = []
    
    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validation"):
            seq_encodings, attention_masks, loss_masks, run_times, labels, sample_idxs = batch
            
            seq_encodings = seq_encodings.to(device)
            attention_masks = attention_masks.to(device)
            loss_masks = loss_masks.to(device)
            run_times = run_times.to(device)
            labels = labels.to(device)
            
            predictions = model((seq_encodings, attention_masks, loss_masks, run_times))
            
            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    
    all_predictions = np.array(all_predictions)
    all_labels = np.array(all_labels)
    
    # ãƒ‡ãƒãƒƒã‚°: äºˆæ¸¬å€¤ã¨ãƒ©ãƒ™ãƒ«ã®çµ±è¨ˆã‚’å‡ºåŠ›
    print(f"\nğŸ” æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ:")
    print(f"   äºˆæ¸¬å€¤ã®ç¯„å›²: [{all_predictions.min():.4f}, {all_predictions.max():.4f}]")
    print(f"   äºˆæ¸¬å€¤ã®å¹³å‡: {all_predictions.mean():.4f}")
    print(f"   äºˆæ¸¬å€¤ãŒ0ä»¥ä¸‹ã®æ•°: {(all_predictions <= 0).sum()} / {len(all_predictions)}")
    print(f"   ãƒ©ãƒ™ãƒ«ã®ç¯„å›²: [{all_labels.min():.4f}, {all_labels.max():.4f}]")
    print(f"   ãƒ©ãƒ™ãƒ«ã®å¹³å‡: {all_labels.mean():.4f}")
    print(f"   ãƒ©ãƒ™ãƒ«ãŒ0ä»¥ä¸‹ã®æ•°: {(all_labels <= 0).sum()} / {len(all_labels)}")
    print()
    
    # äºˆæ¸¬å€¤ãŒ0ä»¥ä¸‹ã®å ´åˆã€æœ€å°å€¤ã‚’è¨­å®šï¼ˆQ-Errorè¨ˆç®—ã®ãŸã‚ï¼‰
    # ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ã®å®Ÿè¡Œæ™‚é–“ã¯100msï¼ˆ0.1ç§’ï¼‰ï½30ç§’ã®ç¯„å›²
    # PostgreSQLã®QErrorãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆ0.1ï¼‰ã«åˆã‚ã›ã‚‹
    min_val = 0.1  # 0.1ç§’ = 100ãƒŸãƒªç§’
    all_predictions = np.clip(all_predictions, min_val, np.inf)
    all_labels = np.clip(all_labels, min_val, np.inf)
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
    # QError ã¨ RMSE ã¯ Metric ã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã¦ã„ã‚‹ã®ã§ evaluate_metric ã‚’ä½¿ç”¨
    # QErrorã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆmin_valã¯0.1ãªã®ã§ã€ãã‚Œã«åˆã‚ã›ã‚‹
    q_error_metric = QError(min_val=min_val)
    rmse_metric = RMSE()
    
    # evaluate_metric ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™
    q_error_value = q_error_metric.evaluate_metric(labels=all_labels, preds=all_predictions)
    rmse_value = rmse_metric.evaluate_metric(labels=all_labels, preds=all_predictions)
    
    # Noneã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if q_error_value is None or np.isnan(q_error_value) or np.isinf(q_error_value):
        q_error_value = float('inf')
    if rmse_value is None or np.isnan(rmse_value) or np.isinf(rmse_value):
        rmse_value = float('inf')
    
    return q_error_value, rmse_value


def generate_feature_statistics_from_plans(
    plan_files: list[str],
    output_path: Path,
    plan_features: list[str],
    max_plans_per_file: Optional[int] = None
) -> Path:
    """
    Trinoãƒ—ãƒ©ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç‰¹å¾´é‡çµ±è¨ˆã‚’ç”Ÿæˆã—ã¦JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    
    Args:
        plan_files: Trino EXPLAIN ANALYZE .txtãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
        output_path: çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜å…ˆãƒ‘ã‚¹
        plan_features: åé›†ã™ã‚‹ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆï¼ˆä¾‹: ["op_name", "est_card"]ï¼‰
        max_plans_per_file: ãƒ•ã‚¡ã‚¤ãƒ«ã‚ãŸã‚Šã®æœ€å¤§ãƒ—ãƒ©ãƒ³æ•°
    
    Returns:
        ä¿å­˜ã•ã‚ŒãŸçµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    print("=" * 80)
    print("ç‰¹å¾´é‡çµ±è¨ˆã®ç”Ÿæˆ")
    print("=" * 80)
    print(f"ãƒ—ãƒ©ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«: {plan_files}")
    print(f"åé›†ã™ã‚‹ç‰¹å¾´é‡: {plan_features}")
    print()
    
    # ãƒ—ãƒ©ãƒ³ã‚’èª­ã¿è¾¼ã¿
    print("ğŸ“‚ ãƒ—ãƒ©ãƒ³ã®èª­ã¿è¾¼ã¿ä¸­...")
    all_plans = load_plans_from_files(plan_files, max_plans_per_file)
    print(f"âœ… {len(all_plans)} å€‹ã®ãƒ—ãƒ©ãƒ³ã‚’èª­ã¿è¾¼ã¿å®Œäº†\n")
    
    # ç‰¹å¾´é‡ã®å€¤ã‚’åé›†
    print("ğŸ“Š ç‰¹å¾´é‡ã®å€¤ã‚’åé›†ä¸­...")
    value_dict = collections.defaultdict(list)
    
    def collect_features_recursively(node):
        """å†å¸°çš„ã«ãƒãƒ¼ãƒ‰ã‹ã‚‰ç‰¹å¾´é‡ã‚’åé›†"""
        if hasattr(node, 'plan_parameters'):
            params = node.plan_parameters
            if isinstance(params, dict):
                # dict ã®å ´åˆ
                for feat in plan_features:
                    if feat in params:
                        value = params[feat]
                        if value is not None:
                            value_dict[feat].append(value)
            else:
                # SimpleNamespace ã®å ´åˆ
                for feat in plan_features:
                    # Trinoå›ºæœ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°
                    if feat == "est_card":
                        # est_card ã¯ est_rows ã‹ã‚‰å–å¾—
                        value = getattr(params, "est_rows", None)
                        if value is not None:
                            value_dict[feat].append(value)
                    elif feat == "est_cost":
                        # est_cost ã¯ est_cpu ã‚’å„ªå…ˆï¼ˆEstimatesã®cpuå€¤ã€æ¨å®šå€¤ãªã®ã§ã‚ˆã‚Šé©åˆ‡ï¼‰
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: est_cpuãŒãªã„å ´åˆã¯act_cpu_timeã‚’ä½¿ç”¨ã€ãã‚Œã‚‚ãªã‘ã‚Œã°act_scheduled_timeã€ãã‚Œã‚‚ãªã‘ã‚Œã°0.0
                        value = getattr(params, "est_cpu", None)
                        if value is None:
                            value = getattr(params, "act_cpu_time", None)
                        if value is None:
                            value = getattr(params, "act_scheduled_time", None)
                        if value is None:
                            value = 0.0
                        value_dict[feat].append(value)
                    else:
                        # ãã®ä»–ã®ç‰¹å¾´é‡
                        if hasattr(params, feat):
                            value = getattr(params, feat)
                            if value is not None:
                                value_dict[feat].append(value)
        
        # å­ãƒãƒ¼ãƒ‰ã‚‚å†å¸°çš„ã«å‡¦ç†
        for child in node.children:
            collect_features_recursively(child)
    
    for plan in tqdm(all_plans, desc="ãƒ—ãƒ©ãƒ³å‡¦ç†"):
        collect_features_recursively(plan)
    
    print()
    
    # çµ±è¨ˆã‚’è¨ˆç®—
    print("ğŸ“ˆ çµ±è¨ˆã‚’è¨ˆç®—ä¸­...")
    statistics_dict = {}
    
    for feat_name, values in value_dict.items():
        values = [v for v in values if v is not None]
        if len(values) == 0:
            continue
        
        # æ•°å€¤å‹ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        if all([isinstance(v, (int, float)) for v in values]):
            # æ•°å€¤å‹: RobustScaler ã‚’ä½¿ç”¨
            scaler = RobustScaler()
            np_values = np.array(values, dtype=np.float32).reshape(-1, 1)
            scaler.fit(np_values)
            
            statistics_dict[feat_name] = {
                "max": float(np_values.max()),
                "scale": float(scaler.scale_.item()),
                "center": float(scaler.center_.item()),
                "type": str(FeatureType.numeric)
            }
        else:
            # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«å‹: value_dict ã‚’ä½œæˆ
            unique_values = sorted(set(str(v) for v in values))
            statistics_dict[feat_name] = {
                "value_dict": {v: idx for idx, v in enumerate(unique_values)},
                "no_vals": len(unique_values),
                "type": str(FeatureType.categorical)
            }
    
    # æŒ‡å®šã•ã‚ŒãŸç‰¹å¾´é‡ã§ã€åé›†ã•ã‚Œãªã‹ã£ãŸã‚‚ã®ï¼ˆTrinoã«ã¯å­˜åœ¨ã—ãªã„ç‰¹å¾´é‡ï¼‰ã‚’è¿½åŠ 
    for feat_name in plan_features:
        if feat_name not in statistics_dict:
            # ç‰¹å¾´é‡ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§çµ±è¨ˆã‚’è¿½åŠ 
            if feat_name == 'est_cost':
                # est_costã¯Trinoã«ã¯ãªã„ã®ã§ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤0ã§è¿½åŠ 
                statistics_dict[feat_name] = {
                    "max": 0.0,
                    "scale": 1.0,  # ã‚¹ã‚±ãƒ¼ãƒ«1.0ã§0ã‚’ä¸­å¿ƒã«
                    "center": 0.0,
                    "type": str(FeatureType.numeric)
                }
                print(f"   âš ï¸  {feat_name} ãŒTrinoãƒ—ãƒ©ãƒ³ã«å­˜åœ¨ã—ãªã„ãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤0ã§è¿½åŠ ã—ã¾ã—ãŸ")
            else:
                # ãã®ä»–ã®æ¬ æç‰¹å¾´é‡ã‚‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§è¿½åŠ 
                statistics_dict[feat_name] = {
                    "max": 0.0,
                    "scale": 1.0,
                    "center": 0.0,
                    "type": str(FeatureType.numeric)
                }
                print(f"   âš ï¸  {feat_name} ãŒåé›†ã•ã‚Œãªã‹ã£ãŸãŸã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤0ã§è¿½åŠ ã—ã¾ã—ãŸ")
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w') as f:
        json.dump(statistics_dict, f, indent=2)
    
    print(f"âœ… çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜: {output_path}")
    print(f"   ç‰¹å¾´é‡æ•°: {len(statistics_dict)}")
    print()
    
    return output_path


def run(args) -> int:
    """ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ"""
    print("=" * 80)
    print("DACE Model Training for Trino")
    print(f"Mode: {args.mode}")
    print("=" * 80)
    print()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
    device = torch.device(args.device)
    print(f"Using device: {device}")
    print()
    
    # train_multi_allãƒ¢ãƒ¼ãƒ‰ã®å‡¦ç†
    if args.mode == 'train_multi_all':
        return run_train_multi_all(args, output_dir, device)
    
    # å¾“æ¥ã®trainãƒ¢ãƒ¼ãƒ‰
    if not args.workload_runs:
        raise ValueError("--workload_runs is required for train mode")
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆfeaturizationã‚’å…ˆã«ä½œæˆï¼‰
    featurization = DACEFeaturization()
    model_config = DACEModelConfig(
        batch_size=args.batch_size,
        hidden_dim=args.hidden_dim,
        node_length=args.node_length,
        pad_length=args.pad_length,
        max_runtime=args.max_runtime,
        loss_weight=args.loss_weight,
        num_workers=args.num_workers,
        device=device,
        loss_class_name='DaceLoss',
        cap_training_samples=args.cap_training_samples,
        featurization=featurization,
        optimizer_kwargs=dict(lr=args.learning_rate)
    )
    
    # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
    statistics_file = Path(args.statistics_file) if args.statistics_file else None
    
    if statistics_file is None or not statistics_file.exists():
        # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯å­˜åœ¨ã—ãªã„å ´åˆã¯è‡ªå‹•ç”Ÿæˆ
        # --train_filesãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°--workload_runsã‹ã‚‰.txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        stat_files = args.train_files
        if not stat_files:
            # --workload_runsã‹ã‚‰.txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŠ½å‡º
            stat_files = [f for f in args.workload_runs if Path(f).suffix.lower() == '.txt']
        
        if not stat_files:
            raise ValueError(
                "çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„:\n"
                "  1. --statistics_file: æ—¢å­˜ã®çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹\n"
                "  2. --train_files: çµ±è¨ˆç”Ÿæˆç”¨ã®Trinoãƒ—ãƒ©ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.txtï¼‰ã®ãƒ‘ã‚¹\n"
                "  3. --workload_runs ã« .txt ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å«ã‚ã‚‹ï¼ˆè‡ªå‹•çš„ã«çµ±è¨ˆç”Ÿæˆã«ä½¿ç”¨ã•ã‚Œã¾ã™ï¼‰"
            )
        
        # è‡ªå‹•ç”Ÿæˆã™ã‚‹çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        auto_stats_path = output_dir / 'feature_statistics.json'
        
        # çµ±è¨ˆã‚’ç”Ÿæˆï¼ˆfeaturizationã§æŒ‡å®šã•ã‚ŒãŸå…¨ã¦ã®ç‰¹å¾´é‡ã‚’å«ã‚ã‚‹ï¼‰
        # Trinoã«ã¯ãªã„ç‰¹å¾´é‡ï¼ˆest_costï¼‰ã‚‚çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã«å«ã‚ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤0ã§å‡¦ç†ã™ã‚‹
        plan_features = list(featurization.PLAN_FEATURES)
        
        # Trinoå›ºæœ‰ã®ãƒãƒƒãƒ”ãƒ³ã‚°: est_card ã¯ est_rows ã‹ã‚‰å–å¾—
        if 'est_card' not in plan_features and 'est_rows' in plan_features:
            # est_rows ãŒã‚ã‚Œã° est_card ã«å¤‰æ›ã—ã¦å‡¦ç†
            pass  # çµ±è¨ˆç”Ÿæˆæ™‚ã«é©åˆ‡ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã‚‹
        
        generate_feature_statistics_from_plans(
            plan_files=stat_files,
            output_path=auto_stats_path,
            plan_features=plan_features,
            max_plans_per_file=args.max_plans_per_file
        )
        
        # generate_feature_statistics_from_plans å†…ã§æ—¢ã«ä¸è¶³ã—ã¦ã„ã‚‹ç‰¹å¾´é‡ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã®ã§ã€
        # ã“ã“ã§ã¯ç¢ºèªã®ã¿
        statistics_file = auto_stats_path
    else:
        print(f"æ—¢å­˜ã®çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨: {statistics_file}")
        print()
    
    # ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰è¨­å®š
    train_workload_runs = [Path(p) for p in args.workload_runs]
    test_workload_runs = [Path(p) for p in args.test_workload_runs] if args.test_workload_runs else []
    
    workload_runs = WorkloadRuns(
        train_workload_runs=train_workload_runs,
        test_workload_runs=test_workload_runs
    )
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼è¨­å®š
    dataloader_options = DataLoaderOptions(
        shuffle=True,
        val_ratio=args.val_ratio,
        pin_memory=(device.type == 'cuda')
    )
    
    print("Creating dataloaders...")
    feature_statistics, train_loader, val_loader, test_loaders = create_dace_dataloader(
        statistics_file=statistics_file,
        model_config=model_config,
        workload_runs=workload_runs,
        dataloader_options=dataloader_options
    )
    
    print(f"Training batches: {len(train_loader)}")
    if val_loader:
        print(f"Validation batches: {len(val_loader)}")
    if test_loaders:
        print(f"Test loaders: {len(test_loaders)}")
    print()
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    print("Creating DACE model...")
    model = DACELora(config=model_config)
    model.to(device)
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)
    
    # è¨“ç·´ãƒ«ãƒ¼ãƒ—
    print("=" * 80)
    print("Starting training...")
    print("=" * 80)
    print()
    
    best_q_error = float('inf')
    
    for epoch in range(1, args.epochs + 1):
        # è¨“ç·´
        train_loss = train_epoch(model, train_loader, optimizer, device, epoch)
        
        # æ¤œè¨¼
        if val_loader and epoch % args.log_every == 0:
            q_error, rmse = validate(model, val_loader, device)
            
            print(f"Epoch {epoch}/{args.epochs}")
            print(f"  Train Loss: {train_loss:.4f}")
            print(f"  Val Q-Error: {q_error:.4f}")
            print(f"  Val RMSE: {rmse:.4f}")
            print()
            
            # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
            if q_error < best_q_error:
                best_q_error = q_error
                checkpoint_path = output_dir / 'best_model.pt'
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'q_error': q_error,
                    'rmse': rmse,
                }, checkpoint_path)
                print(f"  âœ“ Saved best model (Q-Error: {q_error:.4f})")
                print()
        
        # å®šæœŸçš„ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        if epoch % args.save_every == 0:
            checkpoint_path = output_dir / f'checkpoint_epoch_{epoch}.pt'
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
            }, checkpoint_path)
    
    # ãƒ†ã‚¹ãƒˆè©•ä¾¡
    if test_loaders:
        print("=" * 80)
        print("Testing...")
        print("=" * 80)
        
        for i, test_loader in enumerate(test_loaders):
            q_error, rmse = validate(model, test_loader, device)
            print(f"Test Loader {i+1}:")
            print(f"  Q-Error: {q_error:.4f}")
            print(f"  RMSE: {rmse:.4f}")
            print()
    
    print("=" * 80)
    print("Training completed!")
    print(f"Best validation Q-Error: {best_q_error:.4f}")
    print(f"Model saved to: {output_dir}")
    print("=" * 80)
    
    return 0


def run_train_multi_all(args, output_dir: Path, device: torch.device) -> int:
    """20å€‹ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦leave-one-out validationã‚’å®Ÿè¡Œ"""
    # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹20å€‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆé †ï¼‰
    ALL_DATASETS = [
        'accidents', 'airline', 'baseball', 'basketball', 'carcinogenesis',
        'consumer', 'credit', 'employee', 'fhnk', 'financial', 'geneea',
        'genome', 'hepatitis', 'imdb', 'movielens', 'seznam', 'ssb',
        'tournament', 'tpc_h', 'walmart'
    ]
    
    plans_dir = Path(args.plans_dir)
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç¢ºèª
    txt_files = sorted([p for p in plans_dir.glob('*.txt')])
    available_datasets = set()
    for p in txt_files:
        stem = p.stem  # .txtã‚’é™¤ã„ãŸãƒ•ã‚¡ã‚¤ãƒ«å
        parts = stem.split('_')
        # æœ€é•·ãƒãƒƒãƒ: ALL_DATASETSã‹ã‚‰æœ€é•·ã®ä¸€è‡´ã‚’æ¢ã™ï¼ˆtpc_hãªã©ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã«å¯¾å¿œï¼‰
        matched_dataset = None
        for i in range(len(parts), 0, -1):
            candidate = '_'.join(parts[:i])
            if candidate in ALL_DATASETS:
                matched_dataset = candidate
                break
        if matched_dataset:
            available_datasets.add(matched_dataset)
    
    available_datasets = sorted(list(available_datasets))
    print(f"\n{'='*80}")
    print(f"Leave-One-Out Validation for All Datasets (DACE)")
    print(f"{'='*80}")
    print(f"åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(available_datasets)} / {len(ALL_DATASETS)}")
    print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {', '.join(available_datasets)}")
    print(f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print(f"{'='*80}\n")
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    featurization = DACEFeaturization()
    plan_features = list(featurization.PLAN_FEATURES)
    
    # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¤ã„ã¦è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    results_summary = []
    
    for idx, test_dataset in enumerate(available_datasets, 1):
        print(f"\n{'#'*80}")
        print(f"# [{idx}/{len(available_datasets)}] Testing dataset: {test_dataset}")
        print(f"{'#'*80}\n")
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™
            train_files = []
            test_files = []
            
            for p in txt_files:
                stem = p.stem  # .txtã‚’é™¤ã„ãŸãƒ•ã‚¡ã‚¤ãƒ«å
                parts = stem.split('_')
                # æœ€é•·ãƒãƒƒãƒ: ALL_DATASETSã‹ã‚‰æœ€é•·ã®ä¸€è‡´ã‚’æ¢ã™
                matched_dataset = None
                for i in range(len(parts), 0, -1):
                    candidate = '_'.join(parts[:i])
                    if candidate in ALL_DATASETS:
                        matched_dataset = candidate
                        break
                
                if matched_dataset == test_dataset:
                    test_files.append(p)
                elif matched_dataset and matched_dataset in available_datasets:
                    train_files.append(p)
            
            if not train_files or not test_files:
                print(f"âš ï¸  {test_dataset}: è¨“ç·´ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                results_summary.append({
                    'test_dataset': test_dataset,
                    'status': 'skipped',
                    'reason': 'missing files'
                })
                continue
            
            # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ï¼‰
            all_stat_files = train_files + test_files
            model_dir = output_dir / f'models_{test_dataset}'
            model_dir.mkdir(parents=True, exist_ok=True)
            statistics_file = model_dir / 'feature_statistics.json'
            
            generate_feature_statistics_from_plans(
                plan_files=[str(f) for f in all_stat_files],
                output_path=statistics_file,
                plan_features=plan_features,
                max_plans_per_file=args.max_plans_per_file
            )
            
            # ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰è¨­å®š
            train_workload_runs = [p for p in train_files]
            test_workload_runs = [p for p in test_files]
            
            workload_runs = WorkloadRuns(
                train_workload_runs=train_workload_runs,
                test_workload_runs=test_workload_runs
            )
            
            # ãƒ¢ãƒ‡ãƒ«è¨­å®š
            model_config = DACEModelConfig(
                batch_size=args.batch_size,
                hidden_dim=args.hidden_dim,
                node_length=args.node_length,
                pad_length=args.pad_length,
                max_runtime=args.max_runtime,
                loss_weight=args.loss_weight,
                num_workers=args.num_workers,
                device=device,
                loss_class_name='DaceLoss',
                cap_training_samples=args.cap_training_samples,
                featurization=featurization,
                optimizer_kwargs=dict(lr=args.learning_rate)
            )
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼è¨­å®š
            dataloader_options = DataLoaderOptions(
                shuffle=True,
                val_ratio=args.val_ratio,
                pin_memory=(device.type == 'cuda')
            )
            
            print(f"ğŸ“Š Leave-One-Out Validation [{idx}/{len(available_datasets)}]:")
            print(f"  - Training files: {len(train_files)}")
            print(f"  - Test files: {len(test_files)}")
            print()
            
            print("Creating dataloaders...")
            feature_statistics, train_loader, val_loader, test_loaders = create_dace_dataloader(
                statistics_file=statistics_file,
                model_config=model_config,
                workload_runs=workload_runs,
                dataloader_options=dataloader_options
            )
            
            print(f"Training batches: {len(train_loader)}")
            if val_loader:
                print(f"Validation batches: {len(val_loader)}")
            if test_loaders:
                print(f"Test loaders: {len(test_loaders)}")
            print()
            
            # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
            print("Creating DACE model...")
            model = DACELora(config=model_config)
            model.to(device)
            
            # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
            optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)
            
            # è¨“ç·´ãƒ«ãƒ¼ãƒ—
            print("=" * 80)
            print("Starting training...")
            print("=" * 80)
            print()
            
            best_q_error = float('inf')
            best_epoch = 0
            
            for epoch in range(1, args.epochs + 1):
                # è¨“ç·´
                train_loss = train_epoch(model, train_loader, optimizer, device, epoch)
                
                # æ¤œè¨¼
                if val_loader and epoch % args.log_every == 0:
                    q_error, rmse = validate(model, val_loader, device)
                    
                    print(f"Epoch {epoch}/{args.epochs}")
                    print(f"  Train Loss: {train_loss:.4f}")
                    print(f"  Val Q-Error: {q_error:.4f}")
                    print(f"  Val RMSE: {rmse:.4f}")
                    print()
                    
                    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ä¿å­˜
                    if q_error < best_q_error:
                        best_q_error = q_error
                        best_epoch = epoch
                        checkpoint_path = model_dir / 'best_model.pt'
                        torch.save({
                            'epoch': epoch,
                            'model_state_dict': model.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'q_error': q_error,
                            'rmse': rmse,
                        }, checkpoint_path)
                        print(f"  âœ“ Saved best model (Q-Error: {q_error:.4f})")
                        print()
                
                # å®šæœŸçš„ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
                if epoch % args.save_every == 0:
                    checkpoint_path = model_dir / f'checkpoint_epoch_{epoch}.pt'
                    torch.save({
                        'epoch': epoch,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                    }, checkpoint_path)
            
            # ãƒ†ã‚¹ãƒˆè©•ä¾¡
            test_results = {}
            if test_loaders:
                print("=" * 80)
                print("Testing...")
                print("=" * 80)
                
                all_test_q_errors = []
                all_test_rmses = []
                
                for i, test_loader in enumerate(test_loaders):
                    q_error, rmse = validate(model, test_loader, device)
                    all_test_q_errors.append(q_error)
                    all_test_rmses.append(rmse)
                    print(f"Test Loader {i+1}:")
                    print(f"  Q-Error: {q_error:.4f}")
                    print(f"  RMSE: {rmse:.4f}")
                    print()
                
                test_results = {
                    'test_mean_q_error': float(np.mean(all_test_q_errors)) if all_test_q_errors else None,
                    'test_median_q_error': float(np.median(all_test_q_errors)) if all_test_q_errors else None,
                    'test_mean_rmse': float(np.mean(all_test_rmses)) if all_test_rmses else None,
                    'test_samples': sum(len(loader.dataset) for loader in test_loaders) if test_loaders else 0
                }
                
                # ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜
                results_file = model_dir / 'test_results.json'
                with open(results_file, 'w') as f:
                    json.dump(test_results, f, indent=2)
                print(f"âœ… ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜: {results_file}")
                print()
            
            # çµæœã‚’ä¿å­˜
            results_summary.append({
                'test_dataset': test_dataset,
                'model_dir': str(model_dir),
                'best_val_q_error': float(best_q_error),
                'best_epoch': int(best_epoch),
                **test_results,
                'status': 'completed'
            })
            
            print(f"âœ… [{idx}/{len(available_datasets)}] {test_dataset} ã®è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆå®Œäº†")
            print(f"   ãƒ¢ãƒ‡ãƒ«ä¿å­˜å…ˆ: {model_dir}")
            print()
            
        except Exception as e:
            print(f"âŒ [{idx}/{len(available_datasets)}] {test_dataset} ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ:")
            print(f"   {e}")
            import traceback
            traceback.print_exc()
            results_summary.append({
                'test_dataset': test_dataset,
                'status': 'failed',
                'error': str(e)
            })
            continue
    
    # å…¨ä½“ã®ã‚µãƒãƒªãƒ¼ã‚’ä¿å­˜
    summary_file = output_dir / 'leave_one_out_summary.json'
    with open(summary_file, 'w') as f:
        json.dump({
            'total_datasets': len(available_datasets),
            'completed': len([r for r in results_summary if r['status'] == 'completed']),
            'failed': len([r for r in results_summary if r['status'] == 'failed']),
            'skipped': len([r for r in results_summary if r.get('status') == 'skipped']),
            'results': results_summary
        }, f, indent=2)
    
    print("\n" + "=" * 80)
    print("ğŸ‰ å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®Leave-One-Out Validationå®Œäº†ï¼")
    print("=" * 80)
    print(f"å®Œäº†: {len([r for r in results_summary if r['status'] == 'completed'])}/{len(available_datasets)}")
    print(f"å¤±æ•—: {len([r for r in results_summary if r['status'] == 'failed'])}/{len(available_datasets)}")
    print(f"ã‚¹ã‚­ãƒƒãƒ—: {len([r for r in results_summary if r.get('status') == 'skipped'])}/{len(available_datasets)}")
    print(f"ã‚µãƒãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«: {summary_file}")
    print()
    
    return 0


def main(argv: Optional[Sequence[str]] = None) -> int:
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = build_parser()
    args = parser.parse_args(argv)
    return run(args)


if __name__ == "__main__":
    raise SystemExit(main())

