import collections
import re
from typing import List, Any

import numpy as np
try:
    from tqdm import tqdm
except ImportError:
    # tqdmãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯é€šå¸¸ã®rangeã‚’ä½¿ç”¨
    def tqdm(iterable, *args, **kwargs):
        return iterable

from cross_db_benchmark.benchmark_tools.abstract.plan_parser import AbstractPlanParser
from cross_db_benchmark.benchmark_tools.generate_workload import LogicalOperator
from cross_db_benchmark.benchmark_tools.trino.plan_operator import TrinoPlanOperator
from cross_db_benchmark.benchmark_tools.trino.utils import plan_statistics

# Trinoç‰¹æœ‰ã®æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
# Queuedã®å˜ä½ã¯us/Î¼s/msã®ã„ãšã‚Œã‹ã€Executionã®å˜ä½ã¯ms/s/m/us/Î¼sã®ã„ãšã‚Œã‹
trino_timing_regex = re.compile(r'Queued: ([\d.]+)(?:us|Î¼s|ms)?, Analysis: ([\d.]+)ms, Planning: ([\d.]+)ms, Execution: ([\d.]+)(ms|s|m|us|Î¼s)?')
trino_fragment_regex = re.compile(r'Fragment (\d+) \[(\w+)\]')
trino_cpu_regex = re.compile(r'CPU: ([\d.]+)ms')
trino_scheduled_regex = re.compile(r'Scheduled: ([\d.]+)ms')
trino_blocked_regex = re.compile(r'Blocked ([\d.]+)s \(Input: ([\d.]+)s, Output: ([\d.]+)ns\)')
trino_input_regex = re.compile(r'Input: ([\d,]+) rows \(([\d.]+[KMG]?B)\)')
trino_output_regex = re.compile(r'Output: ([\d,]+) rows \(([\d.]+[KMG]?B)\)')
trino_estimates_regex = re.compile(r'Estimates: \{rows: ([\d,?]+) \(([\d.]+[KMG]?B)\)(?:, cpu: ([\d.]+[KMG]?)?)?(?:, memory: ([\d.]+[KMG]?B))?(?:, network: ([\d.]+[KMG]?B))?\}')


class TrinoPlanParser(AbstractPlanParser):
    """Trinoãƒ—ãƒ©ãƒ³ãƒ‘ãƒ¼ã‚µãƒ¼"""
    
    def __init__(self):
        super().__init__(database_type="trino")
    
    def parse_raw_plan(self, plan_text, analyze=True, parse=True, **kwargs):
        """
        Parse raw Trino EXPLAIN ANALYZE text output.
        
        This is the unified interface implementation for Trino.
        Wraps parse_trino_raw_plan_v2 to provide the standard interface.
        
        Args:
            plan_text: Raw Trino EXPLAIN ANALYZE output as string
            analyze: Whether to extract execution statistics (default: True)
            parse: Whether to parse the plan structure (default: True)
            **kwargs: Additional Trino-specific parameters
        
        Returns:
            tuple: (root_operator, execution_time, planning_time)
        """
        return parse_trino_raw_plan_v2(plan_text, analyze=analyze, parse=parse)
    
    def parse_plans(self, run_stats, min_runtime=100, max_runtime=30000, parse_baseline=False, cap_queries=None,
                   parse_join_conds=False, include_zero_card=False, explain_only=False, **kwargs):
        """Trinoãƒ—ãƒ©ãƒ³ã‚’ä¸€æ‹¬è§£æ"""
        
        # çµ±è¨ˆæƒ…å ±ã®åˆæœŸåŒ–
        parsed_plans = []
        avg_runtimes = []
        no_tables = []
        no_filters = []
        op_perc = collections.defaultdict(int)
        database_stats = {}
        
        # ã‚¯ã‚¨ãƒªæ•°ã®åˆ¶é™
        query_list = run_stats.query_list
        if cap_queries:
            query_list = query_list[:cap_queries]
        
        for q in tqdm(query_list):
            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¯ã‚¨ãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
            if hasattr(q, 'timeout') and q.timeout:
                continue
            
            # ãƒ—ãƒ©ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            if not hasattr(q, 'verbose_plan') or not q.verbose_plan:
                continue
            
            # å®Ÿè¡Œæ™‚é–“ã®ç¢ºèª
            if hasattr(q, 'execution_time') and q.execution_time < min_runtime:
                continue
            
            if hasattr(q, 'execution_time') and q.execution_time > max_runtime:
                continue
            
            try:
                # ãƒ—ãƒ©ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
                plan_lines = []
                for l in q.verbose_plan:
                    if isinstance(l, (list, tuple)) and len(l) > 0:
                        plan_lines.append(l[0])
                    elif isinstance(l, str):
                        plan_lines.append(l)
                    else:
                        plan_lines.append('')
                
                plan_text = '\n'.join(plan_lines)
                
                # çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’ä½¿ç”¨
                root_operator, execution_time, planning_time = self.parse_raw_plan(
                    plan_text, analyze=True, parse=True
                )
                
                if root_operator is None:
                    continue
                
                # ãƒ—ãƒ©ãƒ³çµ±è¨ˆã‚’è¨ˆç®—
                stats = plan_statistics(root_operator)
                
                # ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æ§‹ç¯‰
                plan_info = {
                    'root_operator': root_operator,
                    'execution_time': execution_time,
                    'planning_time': planning_time,
                    'stats': stats,
                    'query_id': getattr(q, 'query_id', len(parsed_plans)),
                    'sql': getattr(q, 'sql', ''),
                }
                
                # TrinoPlanOperatorã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«plan_runtimeã‚’è¨­å®š
                root_operator.plan_runtime = execution_time
                root_operator.database_id = getattr(q, 'database_id', 'unknown')
                
                parsed_plans.append(root_operator)
                avg_runtimes.append(execution_time)
                
                # çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°
                if stats['no_tables'] == 0:
                    no_tables.append(len(parsed_plans) - 1)
                
                if stats['no_filters'] == 0:
                    no_filters.append(len(parsed_plans) - 1)
                
                # æ¼”ç®—å­çµ±è¨ˆã‚’æ›´æ–°
                for op_name in stats['operators']:
                    op_perc[op_name] += 1
                    
            except Exception as e:
                print(f"Error parsing query {getattr(q, 'query_id', 'unknown')}: {e}")
                continue
        
        # çµæœã‚’æ§‹ç¯‰ï¼ˆçµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼‰
        parsed_runs = {
            'parsed_plans': parsed_plans,
            'database_stats': getattr(run_stats, 'database_stats', {}),
            'run_kwargs': getattr(run_stats, 'run_kwargs', {})
        }
        
        # çµ±è¨ˆæƒ…å ±ã‚’æ§‹ç¯‰
        stats = {
            'runtimes': str(avg_runtimes),
            'no_tables': str(no_tables),
            'no_filters': str(no_filters),
            'total_plans': len(parsed_plans),
            'avg_runtime': np.mean(avg_runtimes) if avg_runtimes else 0,
            'min_runtime': np.min(avg_runtimes) if avg_runtimes else 0,
            'max_runtime': np.max(avg_runtimes) if avg_runtimes else 0,
            'no_tables_count': len(no_tables),
            'no_filters_count': len(no_filters),
            'operator_percentages': dict(op_perc)
        }
        
        return parsed_runs, stats
    
    def parse_single_plan(self, plan_text, analyze=True, parse=True, **kwargs):
        """å˜ä¸€ã®Trinoãƒ—ãƒ©ãƒ³ã‚’è§£æ"""
        try:
            root_operator, execution_time, planning_time = self.parse_raw_plan(
                plan_text, analyze=analyze, parse=parse
            )
            return root_operator
        except Exception as e:
            print(f"Error parsing single plan: {e}")
            return None
    
    def parse_explain_analyze_file(
        self,
        file_path: str,
        min_runtime: float = 100,
        max_runtime: float = 30000,
        **kwargs
    ) -> tuple[List[Any], List[float]]:
        """
        Parse EXPLAIN ANALYZE results from a text file (Trino-specific implementation).
        
        This method overrides the base implementation to ensure join_conds are set.
        """
        parsed_plans, runtimes = super().parse_explain_analyze_file(
            file_path, min_runtime=min_runtime, max_runtime=max_runtime, **kwargs
        )
        
        # å„ãƒ—ãƒ©ãƒ³ã«join_condsã¨plan_runtimeã‚’è¨­å®š
        for i, plan in enumerate(parsed_plans):
            if not hasattr(plan, 'join_conds') or plan.join_conds is None:
                join_conds = extract_join_conditions_trino(plan)
                plan.join_conds = join_conds
            
            # plan_runtimeã‚’è¨­å®šï¼ˆruntimesãƒªã‚¹ãƒˆã‹ã‚‰ï¼‰
            if i < len(runtimes):
                plan.plan_runtime = runtimes[i]
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µãƒ³ãƒ—ãƒ«ã¨ã‚«ãƒ©ãƒ çµ±è¨ˆãŒæä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€sample_vecã‚’ç”Ÿæˆ
        table_samples = kwargs.get('table_samples')
        col_stats = kwargs.get('col_stats')
        
        # ã‚«ãƒ©ãƒ IDãƒãƒƒãƒ”ãƒ³ã‚°ã‚‚æä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ã‚«ãƒ©ãƒ IDã«å¤‰æ›
        column_id_mapping = kwargs.get('column_id_mapping')
        partial_column_name_mapping = kwargs.get('partial_column_name_mapping')
        table_id_mapping = kwargs.get('table_id_mapping')
        
        if table_samples is not None and col_stats is not None:
            # augment_sampleé–¢æ•°ã‚’ç›´æ¥ä½¿ç”¨ã—ã¦sample_vecã‚’ç”Ÿæˆ
            from models.workload_driven.preprocessing.sample_vectors_trino import augment_sample
            
            for plan in parsed_plans:
                try:
                    # ãƒ—ãƒ©ãƒ³ãƒ„ãƒªãƒ¼ã‚’å†å¸°çš„ã«èµ°æŸ»ã—ã¦sample_vecã‚’ç”Ÿæˆ
                    augment_sample(table_samples, col_stats, plan)
                except Exception as e:
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶šè¡Œï¼ˆsample_vecã¯ç©ºã®ã¾ã¾ï¼‰
                    # ãƒ‡ãƒãƒƒã‚°ç”¨: æœ€åˆã®æ•°å›ã®ã¿ã‚¨ãƒ©ãƒ¼ã‚’å‡ºåŠ›
                    if not hasattr(TrinoPlanParser, '_sample_vec_error_count'):
                        TrinoPlanParser._sample_vec_error_count = 0
                    if TrinoPlanParser._sample_vec_error_count < 3:
                        print(f"Warning: Failed to generate sample_vec in parse_explain_analyze_file: {e}")
                        import traceback
                        traceback.print_exc()
                        TrinoPlanParser._sample_vec_error_count += 1
        
        # ã‚«ãƒ©ãƒ IDãƒãƒƒãƒ”ãƒ³ã‚°ãŒæä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ã‚«ãƒ©ãƒ IDã«å¤‰æ›
        if column_id_mapping is not None and partial_column_name_mapping is not None:
            for plan in parsed_plans:
                try:
                    # parse_columns_bottom_upã‚’å‘¼ã³å‡ºã—ã¦ã‚«ãƒ©ãƒ IDã«å¤‰æ›
                    plan.parse_columns_bottom_up(
                        column_id_mapping=column_id_mapping,
                        partial_column_name_mapping=partial_column_name_mapping,
                        table_id_mapping=table_id_mapping if table_id_mapping else {},
                        alias_dict={},
                        table_samples=None,  # sample_vecã¯æ—¢ã«ç”Ÿæˆæ¸ˆã¿
                        col_stats=None
                    )
                except Exception:
                    # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶šè¡Œ
                    pass
        
        return parsed_plans, runtimes


def parse_trino_plan_simple(plan_text):
    """Trinoãƒ—ãƒ©ãƒ³ã‚’ç°¡æ˜“çš„ã«è§£æ"""
    lines = plan_text.strip().split('\n')
    
    # ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã‚’æŠ½å‡º
    queued_time = 0
    analysis_time = 0
    planning_time = 0
    execution_time = 0
    
    for line in lines:
        timing_match = trino_timing_regex.search(line)
        if timing_match:
            queued_value = float(timing_match.group(1))
            # Queuedã®å˜ä½ã‚’ç¢ºèªï¼ˆæ­£è¦è¡¨ç¾ã§ã¯ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã¦ã„ãªã„ã®ã§ã€msã‹us/Î¼sã‹ã‚’åˆ¤å®šï¼‰
            queued_str = timing_match.group(0)
            if 'ms' in queued_str:
                queued_time = queued_value  # ms
            else:
                queued_time = queued_value / 1000  # us/Î¼s to ms
            
            analysis_time = float(timing_match.group(2))
            planning_time = float(timing_match.group(3))
            execution_time = float(timing_match.group(4))
            execution_unit = timing_match.group(5) if timing_match.group(5) else 'ms'
            
            # å®Ÿè¡Œæ™‚é–“ã®å˜ä½ã‚’ãƒŸãƒªç§’ã«çµ±ä¸€
            if execution_unit == 's':
                execution_time = execution_time * 1000  # s to ms
            elif execution_unit == 'm':
                execution_time = execution_time * 60000  # m to ms
            elif execution_unit in ('us', 'Î¼s'):
                execution_time = execution_time / 1000  # us/Î¼s to ms
            # execution_unit == 'ms' ã®å ´åˆã¯ãã®ã¾ã¾
            
            break
    
    # Fragmentã‚’æ¤œå‡º
    fragments = []
    current_fragment = None
    
    for line in lines:
        fragment_match = trino_fragment_regex.search(line)
        if fragment_match:
            if current_fragment:
                fragments.append(current_fragment)
            current_fragment = {
                'id': fragment_match.group(1),  # æ–‡å­—åˆ—ã¨ã—ã¦ä¿æŒ
                'type': fragment_match.group(2),
                'lines': [line]
            }
        elif current_fragment:
            current_fragment['lines'].append(line)
    
    if current_fragment:
        fragments.append(current_fragment)
    
    # ã™ã¹ã¦ã®Fragmentã‹ã‚‰æ¼”ç®—å­ã‚’æŠ½å‡º
    all_operators = []
    for fragment in fragments:
        fragment_operators = extract_operators_from_fragment(fragment)
        # Fragmentæƒ…å ±ã‚’æ¼”ç®—å­ã«è¿½åŠ 
        for operator in fragment_operators:
            operator['fragment_id'] = fragment['id']
            operator['fragment_type'] = fragment['type']
            
            # TrinoPlanOperatorã‚’ä½œæˆã—ã¦è©³ç´°è§£æ
            from cross_db_benchmark.benchmark_tools.trino.plan_operator import TrinoPlanOperator
            trino_operator = TrinoPlanOperator(operator['lines'])
            trino_operator.plan_parameters['op_name'] = operator['name']
            trino_operator.plan_parameters['fragment_id'] = fragment['id']
            trino_operator.plan_parameters['fragment_type'] = fragment['type']
            trino_operator.parse_lines_recursively()
            
            # è§£æã•ã‚ŒãŸæƒ…å ±ã‚’æ¼”ç®—å­ã«è¿½åŠ 
            operator.update(trino_operator.plan_parameters)
            
        all_operators.extend(fragment_operators)
    
    return all_operators, execution_time, planning_time


def extract_operators_from_fragment(fragment):
    """Fragmentã‹ã‚‰æ¼”ç®—å­ã‚’æŠ½å‡º"""
    operators = []
    current_operator = None
    current_lines = []
    
    for line in fragment['lines']:
        stripped = line.strip()
        
        # æ¼”ç®—å­ã®é–‹å§‹ã‚’æ¤œå‡ºï¼ˆã‚ˆã‚Šæ­£ç¢ºãªæ¡ä»¶ï¼‰
        if is_operator_line(stripped):
            # å‰ã®æ¼”ç®—å­ã‚’ä¿å­˜
            if current_operator:
                current_operator['lines'] = current_lines
                operators.append(current_operator)
            
            # æ–°ã—ã„æ¼”ç®—å­ã‚’é–‹å§‹
            depth = count_indent_depth(line)
            current_operator = {
                'name': extract_operator_name(stripped),
                'depth': depth,
                'lines': []
            }
            current_lines = [line]
        else:
            # æ¼”ç®—å­ã®è©³ç´°æƒ…å ±
            if current_operator:
                current_lines.append(line)
    
    # æœ€å¾Œã®æ¼”ç®—å­ã‚’ä¿å­˜
    if current_operator:
        current_operator['lines'] = current_lines
        operators.append(current_operator)
    return operators


def is_operator_line(line):
    """è¡ŒãŒæ¼”ç®—å­ã®é–‹å§‹è¡Œã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    if not line:
        return False
    
    # æ˜ã‚‰ã‹ã«æ¼”ç®—å­ã§ãªã„è¡Œã‚’é™¤å¤–
    exclude_patterns = [
        'Fragment', 'CPU:', 'Scheduled:', 'Blocked', 'Peak Memory:',
        'Output buffer', 'Task output', 'Task input', 'Output layout:',
        'Output partitioning:', 'metrics:', 'Input avg.:', 'Input std.dev.:',
        'Distribution:', 'dynamicFilterAssignments', 'Dynamic filters:',
        'Left (probe)', 'Right (build)', 'Reorder joins cost'
    ]
    
    for pattern in exclude_patterns:
        if line.startswith(pattern):
            return False
    
    # æ¼”ç®—å­ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
    operator_patterns = [
        r'^\w+\[',  # Aggregate[type = FINAL]
        r'^\w+\s*$',  # TableScan
        r'^â””â”€\s*\w+',  # â””â”€ LocalExchange
        r'^â”œâ”€\s*\w+',  # â”œâ”€ ScanFilter
        r'^\w+\[.*\]',  # ScanFilterProject[table = ...]
    ]
    
    for pattern in operator_patterns:
        if re.match(pattern, line):
            return True
    
    return False


def extract_operator_name(line):
    """è¡Œã‹ã‚‰æ¼”ç®—å­åã‚’æŠ½å‡º"""
    # è¡Œã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    line = line.strip()
    
    # æ¼”ç®—å­åã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
    patterns = [
        r'^â””â”€\s*(\w+)',  # â””â”€ LocalExchange
        r'^â”œâ”€\s*(\w+)',  # â”œâ”€ ScanFilter
        r'^(\w+)\[',     # Aggregate[type = FINAL]
        r'^(\w+)\s*$',   # TableScan
        r'^(\w+)\[.*\]', # ScanFilterProject[table = ...]
    ]
    
    for pattern in patterns:
        match = re.search(pattern, line)
        if match:
            return match.group(1)
    
    return 'Unknown'


def count_indent_depth(line):
    """è¡Œã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆæ·±åº¦ã‚’è¨ˆç®—"""
    # ã‚¹ãƒšãƒ¼ã‚¹ã¨ã‚¿ãƒ–ã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’è¨ˆç®—
    space_indent = len(line) - len(line.lstrip())
    
    # Trinoã®éšå±¤è¨˜å·ï¼ˆâ””â”€, â”œâ”€ï¼‰ã‚’è€ƒæ…®
    if 'â””â”€' in line:
        # â””â”€ ã¯å­ãƒãƒ¼ãƒ‰ã‚’ç¤ºã™ï¼ˆè¦ªã‚ˆã‚Š1ãƒ¬ãƒ™ãƒ«æ·±ã„ï¼‰
        return space_indent + 1
    elif 'â”œâ”€' in line:
        # â”œâ”€ ã¯å…„å¼Ÿãƒãƒ¼ãƒ‰ã‚’ç¤ºã™ï¼ˆåŒã˜ãƒ¬ãƒ™ãƒ«ï¼‰
        return space_indent
    else:
        # é€šå¸¸ã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ
        return space_indent


def create_trino_plan_operator(operator_info):
    """æ¼”ç®—å­æƒ…å ±ã‹ã‚‰TrinoPlanOperatorã‚’ä½œæˆ"""
    operator = TrinoPlanOperator(operator_info['lines'])
    operator.plan_parameters['op_name'] = operator_info['name']
    operator.plan_parameters['depth'] = operator_info['depth']
    # Fragmentæƒ…å ±ã‚’è¨­å®š
    if 'fragment_id' in operator_info:
        operator.plan_parameters['fragment_id'] = operator_info['fragment_id']
    if 'fragment_type' in operator_info:
        operator.plan_parameters['fragment_type'] = operator_info['fragment_type']
    return operator


def extract_join_conditions_trino(root_operator):
    """Trinoãƒ—ãƒ©ãƒ³ã‹ã‚‰çµåˆæ¡ä»¶ã‚’æŠ½å‡º"""
    join_conds = []
    
    def traverse_plan(node):
        """ãƒ—ãƒ©ãƒ³ã‚’å†å¸°çš„ã«èµ°æŸ»ã—ã¦çµåˆæ¡ä»¶ã‚’åé›†"""
        # ç¾åœ¨ã®ãƒãƒ¼ãƒ‰ãŒJoinæ¼”ç®—å­ã‹ãƒã‚§ãƒƒã‚¯
        op_name = node.plan_parameters.get('op_name', '').lower()
        
        # Joinæ¼”ç®—å­ã®æ¤œå‡ºï¼ˆInnerJoin, LeftJoin, RightJoin, FullJoin, CrossJoinãªã©ï¼‰
        if 'join' in op_name:
            # criteriaãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰çµåˆæ¡ä»¶ã‚’æŠ½å‡º
            # InnerJoin[criteria = (id_upravna_enota = upravna_enota_4), ...]ã®ã‚ˆã†ãªå½¢å¼
            criteria = node.plan_parameters.get('criteria')
            if criteria:
                # æ‹¬å¼§ã‚’é™¤å»ã—ã¦çµåˆæ¡ä»¶ã‚’æ­£è¦åŒ–
                join_cond = criteria.strip('()')
                if join_cond:
                    join_conds.append(join_cond)
            else:
                # filterPredicateã‹ã‚‰çµåˆæ¡ä»¶ã‚’æ¨æ¸¬ï¼ˆçµåˆæ¡ä»¶ã‚‰ã—ã„ãƒ•ã‚£ãƒ«ã‚¿ã‚’æ¢ã™ï¼‰
                filter_condition = node.plan_parameters.get('filter_condition')
                if filter_condition:
                    # ç­‰å·ã‚’å«ã‚€ãƒ•ã‚£ãƒ«ã‚¿ã‚’çµåˆæ¡ä»¶ã¨ã—ã¦æ‰±ã†
                    if '=' in filter_condition and '.' in filter_condition:
                        # ãƒ†ãƒ¼ãƒ–ãƒ«å.ã‚«ãƒ©ãƒ åã®å½¢å¼ã‚’æ¤œå‡º
                        join_cond = filter_condition.strip('()')
                        if join_cond:
                            join_conds.append(join_cond)
        
        # å­ãƒãƒ¼ãƒ‰ã‚’å†å¸°çš„ã«èµ°æŸ»
        for child in node.children:
            traverse_plan(child)
    
    traverse_plan(root_operator)
    return join_conds


def parse_trino_raw_plan_v2(plan_text, analyze=True, parse=True):
    """Trinoã®ç”Ÿãƒ—ãƒ©ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    if not parse:
        # ã‚¿ã‚¤ãƒŸãƒ³ã‚°æƒ…å ±ã®ã¿æŠ½å‡º
        lines = plan_text.strip().split('\n')
        execution_time = 0
        planning_time = 0
        
        for line in lines:
            timing_match = trino_timing_regex.search(line)
            if timing_match:
                execution_time = float(timing_match.group(4))
                planning_time = float(timing_match.group(3))
                break
        
        return None, execution_time, planning_time
    
    # ãƒ—ãƒ©ãƒ³ã‚’è§£æ
    all_operators, execution_time, planning_time = parse_trino_plan_simple(plan_text)
    
    if not all_operators:
        return None, execution_time, planning_time
    
    # Fragment 1ã®æ¼”ç®—å­ã‚’ãƒ«ãƒ¼ãƒˆã¨ã—ã¦ã€å…¨Fragmentã®æ¼”ç®—å­ã‚’çµ±åˆ
    root_operators = [op for op in all_operators if op.get('fragment_id') == '1']
    
    if not root_operators:
        return None, execution_time, planning_time
    
    # å…¨Fragmentã®æ¼”ç®—å­ã‚’çµ±åˆï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’å«ã‚€ï¼‰
    all_fragment_operators = all_operators
    
    # æ¼”ç®—å­ã‚’éšå±¤æ§‹é€ ã«å¤‰æ›ï¼ˆå…¨Fragmentã®æ¼”ç®—å­ã‚’å«ã‚€ï¼‰
    root_operator = build_hierarchy(root_operators, all_fragment_operators)
    
    # çµåˆæ¡ä»¶ã‚’æŠ½å‡ºã—ã¦root_operatorã«è¨­å®š
    join_conds = extract_join_conditions_trino(root_operator)
    root_operator.join_conds = join_conds
    
    return root_operator, execution_time, planning_time


def parse_trino_plans(run_stats, min_runtime=100, max_runtime=30000, parse_baseline=False, cap_queries=None,
                      parse_join_conds=False, include_zero_card=False, explain_only=False):
    """Trinoãƒ—ãƒ©ãƒ³ã‚’ä¸€æ‹¬è§£æ"""
    
    # çµ±è¨ˆæƒ…å ±ã®åˆæœŸåŒ–
    parsed_plans = []
    avg_runtimes = []
    no_tables = []
    no_filters = []
    op_perc = collections.defaultdict(int)
    database_stats = {}
    
    # ã‚¯ã‚¨ãƒªæ•°ã®åˆ¶é™
    query_list = run_stats.query_list
    if cap_queries:
        query_list = query_list[:cap_queries]
    
    for q in tqdm(query_list):
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¯ã‚¨ãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
        if hasattr(q, 'timeout') and q.timeout:
            continue
        
        # ãƒ—ãƒ©ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not hasattr(q, 'verbose_plan') or not q.verbose_plan:
            continue
        
        # å®Ÿè¡Œæ™‚é–“ã®ç¢ºèª
        if hasattr(q, 'execution_time') and q.execution_time < min_runtime:
            continue
        
        if hasattr(q, 'execution_time') and q.execution_time > max_runtime:
            continue
        
        try:
            # ãƒ—ãƒ©ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
            plan_lines = []
            for l in q.verbose_plan:
                if isinstance(l, (list, tuple)) and len(l) > 0:
                    plan_lines.append(l[0])
                elif isinstance(l, str):
                    plan_lines.append(l)
                else:
                    plan_lines.append('')
            
            plan_text = '\n'.join(plan_lines)
            
            # ãƒ—ãƒ©ãƒ³ã‚’è§£æ
            root_operator, execution_time, planning_time = parse_trino_raw_plan_v2(
                plan_text, analyze=True, parse=True
            )
            
            if root_operator is None:
                continue
            
            # ãƒ—ãƒ©ãƒ³çµ±è¨ˆã‚’è¨ˆç®—
            stats = plan_statistics(root_operator)
            
            # ãƒ—ãƒ©ãƒ³æƒ…å ±ã‚’æ§‹ç¯‰
            plan_info = {
                'root_operator': root_operator,
                'execution_time': execution_time,
                'planning_time': planning_time,
                'stats': stats,
                'query_id': getattr(q, 'query_id', len(parsed_plans)),
                'sql': getattr(q, 'sql', ''),
            }
            
            # TrinoPlanOperatorã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«plan_runtimeã‚’è¨­å®š
            root_operator.plan_runtime = execution_time
            root_operator.database_id = getattr(q, 'database_id', 'unknown')
            
            parsed_plans.append(root_operator)
            avg_runtimes.append(execution_time)
            
            # çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°
            if stats['no_tables'] == 0:
                no_tables.append(len(parsed_plans) - 1)
            
            if stats['no_filters'] == 0:
                no_filters.append(len(parsed_plans) - 1)
            
            # æ¼”ç®—å­çµ±è¨ˆã‚’æ›´æ–°
            for op_name in stats['operators']:
                op_perc[op_name] += 1
                
        except Exception as e:
            print(f"Error parsing query {getattr(q, 'query_id', 'unknown')}: {e}")
            continue
    
    # çµæœã‚’æ§‹ç¯‰
    parsed_runs = {
        'parsed_plans': parsed_plans,
        'avg_runtimes': avg_runtimes,
        'no_tables': no_tables,
        'no_filters': no_filters,
        'op_perc': dict(op_perc),
        'database': 'trino'
    }
    
    # çµ±è¨ˆæƒ…å ±ã‚’æ§‹ç¯‰
    stats = {
        'total_plans': len(parsed_plans),
        'avg_runtime': np.mean(avg_runtimes) if avg_runtimes else 0,
        'min_runtime': np.min(avg_runtimes) if avg_runtimes else 0,
        'max_runtime': np.max(avg_runtimes) if avg_runtimes else 0,
        'no_tables_count': len(no_tables),
        'no_filters_count': len(no_filters),
        'operator_percentages': dict(op_perc)
    }
    
    return parsed_runs, stats


def build_hierarchy(operators, all_fragment_operators=None):
    """æ¼”ç®—å­ã®ãƒªã‚¹ãƒˆã‚’éšå±¤æ§‹é€ ã«å¤‰æ›ï¼ˆå…¨Fragmentã®æ¼”ç®—å­ã‚’å«ã‚€ï¼‰"""
    if not operators:
        return None
    
    # ãƒ«ãƒ¼ãƒˆæ¼”ç®—å­ã‚’ä½œæˆï¼ˆæœ€åˆã®æ¼”ç®—å­ï¼‰
    root_operator = create_trino_plan_operator(operators[0])
    
    # å­ãƒãƒ¼ãƒ‰ã‚’å†å¸°çš„ã«æ§‹ç¯‰
    build_children(root_operator, operators, 1, operators[0]['depth'])
    
    # å…¨Fragmentã®æ¼”ç®—å­ã‚’çµ±åˆï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’å«ã‚€ï¼‰
    if all_fragment_operators:
        integrate_all_fragments(root_operator, all_fragment_operators)
    
    # æ¼”ç®—å­ã®è©³ç´°è§£æ
    root_operator.parse_lines_recursively()
    
    # å­ãƒãƒ¼ãƒ‰ã®ã‚«ãƒ¼ãƒ‡ã‚£ãƒŠãƒªãƒ†ã‚£ã‚’è¨ˆç®—ã¨output_columnsã®ç”Ÿæˆï¼ˆTrinoç”¨ã«ç°¡ç•¥åŒ–ï¼‰
    try:
        root_operator.parse_columns_bottom_up({}, {}, {}, alias_dict={}, table_samples=None, col_stats=None)
    except (KeyError, ValueError) as e:
        # Trinoã®è¤‡é›‘ãªã‚«ãƒ©ãƒ åã«å¯¾å¿œã™ã‚‹ãŸã‚ã€ã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã¦output_columnsã®ã¿ç”Ÿæˆ
        print(f"âš ï¸  parse_columns_bottom_upã§ã‚¨ãƒ©ãƒ¼ï¼ˆç„¡è¦–ï¼‰: {e}")
        # æ‰‹å‹•ã§output_columnsã‚’ç”Ÿæˆ
        generate_output_columns_manually(root_operator)
    
    return root_operator


def generate_output_columns_manually(node, is_root=True):
    """æ‰‹å‹•ã§output_columnsã‚’ç”Ÿæˆï¼ˆTrinoç”¨ï¼‰- ãƒ«ãƒ¼ãƒˆãƒãƒ¼ãƒ‰ã®ã¿"""
    # ãƒ«ãƒ¼ãƒˆãƒãƒ¼ãƒ‰ã®ã¿ã§output_columnsã‚’ç”Ÿæˆï¼ˆä¸­é–“æ¼”ç®—å­ã®è¤‡é›‘ãªã‚«ãƒ©ãƒ åã‚’é¿ã‘ã‚‹ï¼‰
    if is_root:
        if 'layout' in node.plan_parameters:
            layout = node.plan_parameters['layout']
            if layout:
                try:
                    output_columns = node.parse_output_columns(','.join(layout))
                    node.plan_parameters['output_columns'] = output_columns
                except Exception as e:
                    print(f"âš ï¸  output_columnsç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ï¼ˆç„¡è¦–ï¼‰: {e}")
    
    # å­ãƒãƒ¼ãƒ‰ã‚‚å†å¸°çš„ã«å‡¦ç†ï¼ˆis_root=Falseï¼‰
    for child in node.children:
        generate_output_columns_manually(child, is_root=False)


def integrate_all_fragments(root_operator, all_fragment_operators):
    """å…¨Fragmentã®æ¼”ç®—å­ã‚’çµ±åˆï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«æƒ…å ±ã‚’å«ã‚€ï¼‰"""
    # å­˜åœ¨ã™ã‚‹Fragmentã‚’å‹•çš„ã«æ¤œå‡º
    fragment_ids = set()
    for operator in all_fragment_operators:
        fragment_id = operator.get('fragment_id', '')
        if fragment_id:
            fragment_ids.add(fragment_id)
    
    print(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸFragment: {sorted(fragment_ids)}")
    
    # Fragment 1ä»¥å¤–ã®ã™ã¹ã¦ã®Fragmentã®æ¼”ç®—å­ã‚’è¿½åŠ 
    for operator in all_fragment_operators:
        fragment_id = operator.get('fragment_id', '')
        if fragment_id != '1':  # Fragment 1ä»¥å¤–ã®ã™ã¹ã¦ã®Fragment
            # ãƒ†ãƒ¼ãƒ–ãƒ«æ¼”ç®—å­ã‚’å­ãƒãƒ¼ãƒ‰ã¨ã—ã¦è¿½åŠ 
            child_operator = create_trino_plan_operator(operator)
            root_operator.children.append(child_operator)


def build_children(parent, operators, start_idx, parent_depth):
    """å­ãƒãƒ¼ãƒ‰ã‚’å†å¸°çš„ã«æ§‹ç¯‰"""
    i = start_idx
    while i < len(operators):
        operator = operators[i]
        
        # è¦ªã‚ˆã‚Šæ·±ã„æ¼”ç®—å­ã¯å­ãƒãƒ¼ãƒ‰
        if operator['depth'] > parent_depth:
            child = create_trino_plan_operator(operator)
            parent.children.append(child)
            
            # å­ãƒãƒ¼ãƒ‰ã®å­ã‚’å†å¸°çš„ã«æ§‹ç¯‰
            i = build_children(child, operators, i + 1, operator['depth'])
        else:
            # åŒã˜æ·±åº¦ã¾ãŸã¯æµ…ã„æ·±åº¦ã®æ¼”ç®—å­ã¯å…„å¼Ÿãƒãƒ¼ãƒ‰
            break
    
    return i


def parse_trino_plans_v2(run_stats, min_runtime=100, max_runtime=30000, parse_baseline=False, cap_queries=None,
                        parse_join_conds=False, include_zero_card=False, explain_only=False):
    """Trinoãƒ—ãƒ©ãƒ³ã‚’ä¸€æ‹¬è§£æï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    # ã‚«ãƒ©ãƒ çµ±è¨ˆæƒ…å ±ã®ãƒãƒƒãƒ”ãƒ³ã‚°ä½œæˆ
    column_id_mapping = dict()
    table_id_mapping = dict()
    partial_column_name_mapping = collections.defaultdict(set)
    
    database_stats = run_stats.database_stats
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µã‚¤ã‚ºæƒ…å ±ã‚’ã‚«ãƒ©ãƒ çµ±è¨ˆã«è¿½åŠ 
    table_sizes = dict()
    for table_stat in database_stats.table_stats:
        table_sizes[table_stat.relname] = table_stat.reltuples
    
    for i, column_stat in enumerate(database_stats.column_stats):
        table = column_stat.tablename
        column = column_stat.attname
        column_stat.table_size = table_sizes[table]
        column_id_mapping[(table, column)] = i
        partial_column_name_mapping[column].add(table)
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    for i, table_stat in enumerate(database_stats.table_stats):
        table = table_stat.relname
        table_id_mapping[table] = i
    
    # å€‹åˆ¥ã‚¯ã‚¨ãƒªã®è§£æ
    parsed_plans = []
    avg_runtimes = []
    no_tables = []
    no_filters = []
    op_perc = collections.defaultdict(int)
    
    for q in tqdm(run_stats.query_list):
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¯ã‚¨ãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
        if hasattr(q, 'timeout') and q.timeout:
            continue
        
        alias_dict = dict()
        
        if not explain_only:
            if q.analyze_plans is None:
                continue
            
            if len(q.analyze_plans) == 0:
                continue
            
            # å¹³å‡å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
            ex_times = []
            planning_times = []
            for analyze_plan in q.analyze_plans:
                _, ex_time, planning_time = parse_trino_raw_plan_v2(analyze_plan, analyze=True, parse=False)
                ex_times.append(ex_time)
                planning_times.append(planning_time)
            avg_runtime = sum(ex_times) / len(ex_times)
            
            # ãƒ—ãƒ©ãƒ³ã‚’ãƒ„ãƒªãƒ¼æ§‹é€ ã«è§£æ
            analyze_plan, _, _ = parse_trino_raw_plan_v2(q.analyze_plans[0], analyze=True, parse=True)
            
            if not analyze_plan:
                continue
                
        else:
            avg_runtime = 0
        
        # EXPLAINã®ã¿ã®ãƒ—ãƒ©ãƒ³è§£æ
        if hasattr(q, 'verbose_plan') and q.verbose_plan:
            verbose_plan, _, _ = parse_trino_raw_plan_v2(q.verbose_plan, analyze=False, parse=True)
        else:
            verbose_plan = None
        
        if not explain_only and analyze_plan:
            analyze_plan = analyze_plan
        else:
            analyze_plan = verbose_plan
        
        if not analyze_plan:
            continue
        
        # çµåˆæ¡ä»¶ã‚’æŠ½å‡ºã—ã¦è¨­å®šï¼ˆã¾ã è¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
        if not hasattr(analyze_plan, 'join_conds') or analyze_plan.join_conds is None:
            join_conds = extract_join_conditions_trino(analyze_plan)
            analyze_plan.join_conds = join_conds
        
        # ãƒ—ãƒ©ãƒ³çµ±è¨ˆæƒ…å ±ã‚’åé›†
        stats_result = plan_statistics(analyze_plan)
        tables = stats_result['tables']
        filter_columns = stats_result['filter_columns']
        operators = stats_result['operators']
        
        # ã‚«ãƒ©ãƒ æƒ…å ±ã‚’çµ±è¨ˆæƒ…å ±ã¨ç…§åˆ
        try:
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚µãƒ³ãƒ—ãƒ«ã¨ã‚«ãƒ©ãƒ çµ±è¨ˆã‚’å–å¾—ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰
            table_samples = getattr(run_stats, 'table_samples', None)
            col_stats = database_stats.column_stats if hasattr(database_stats, 'column_stats') else None
            
            analyze_plan.parse_columns_bottom_up(column_id_mapping, partial_column_name_mapping, table_id_mapping,
                                               alias_dict=alias_dict,
                                               table_samples=table_samples,
                                               col_stats=col_stats)
        except Exception as e:
            print(f"Warning: Column parsing failed: {e}")
        
        analyze_plan.plan_runtime = avg_runtime
        
        if not explain_only:
            # çµæœã®ãƒã‚§ãƒƒã‚¯
            if hasattr(analyze_plan, 'min_card') and analyze_plan.min_card() == 0 and not include_zero_card:
                continue
            
            if min_runtime is not None and avg_runtime < min_runtime:
                continue
            
            if avg_runtime > max_runtime:
                continue
        
        # çµ±è¨ˆæƒ…å ±ã‚’åé›†
        avg_runtimes.append(avg_runtime)
        no_tables.append(len(tables))
        for _, op in filter_columns:
            op_perc[op] += 1
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ•°ï¼ˆAND, ORã‚’é™¤ãï¼‰
        no_filters.append(len([fc for fc in filter_columns if fc[0] is not None]))
        
        # SQLæ–‡å­—åˆ—ã‚’ãƒ—ãƒ©ãƒ³ã«è¿½åŠ ï¼ˆå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ï¼‰
        if hasattr(q, 'sql'):
            analyze_plan.sql = q.sql
        else:
            analyze_plan.sql = 'SELECT * FROM unknown'  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        
        parsed_plans.append(analyze_plan)
        
        if cap_queries is not None and len(parsed_plans) >= cap_queries:
            print(f"Parsed {cap_queries} queries. Stopping parsing.")
            break
    
    # çµ±è¨ˆæƒ…å ±ã®å‡ºåŠ›
    if not no_tables == []:
        print(f"Table statistics: "
              f"\n\tmean: {np.mean(no_tables):.1f}"
              f"\n\tmedian: {np.median(no_tables)}"
              f"\n\tmax: {np.max(no_tables)}")
        print("Operators statistics (appear in x% of queries)")
        for op, op_count in op_perc.items():
            print(f"\t{str(op)}: {op_count / len(avg_runtimes) * 100:.0f}%")
        print(f"Runtime statistics: "
              f"\n\tmedian: {np.median(avg_runtimes) / 1000:.2f}s"
              f"\n\tmax: {np.max(avg_runtimes) / 1000:.2f}s"
              f"\n\tmean: {np.mean(avg_runtimes) / 1000:.2f}s")
        print(f"Parsed {len(parsed_plans)} plans ({len(run_stats.query_list) - len(parsed_plans)} had zero-cardinalities "
              f"or were too fast).")
    
    parsed_runs = dict(parsed_plans=parsed_plans, database_stats=database_stats,
                       run_kwargs=run_stats.run_kwargs)
    
    stats = dict(
        runtimes=str(avg_runtimes),
        no_tables=str(no_tables),
        no_filters=str(no_filters)
    )
    
    return parsed_runs, stats
