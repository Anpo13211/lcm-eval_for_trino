"""
Trinoã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®çµ±è¨ˆæƒ…å ±ã‚’åé›†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Usage:
    # ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰å®Ÿè¡Œ
    python -m trino_models.scripts.collect_stats \
        --catalog iceberg \
        --schema imdb \
        --tables name,cast_info
    
çµ±è¨ˆæƒ…å ±ã¯ä»¥ä¸‹ã®æ§‹é€ ã§ä¿å­˜ã•ã‚Œã¾ã™:
    datasets_statistics/
        <catalog>_<schema>/
            column_stats.json  # ã‚«ãƒ©ãƒ ãƒ¬ãƒ™ãƒ«ã®çµ±è¨ˆæƒ…å ±
            table_stats.json   # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ¬ãƒ™ãƒ«ã®çµ±è¨ˆæƒ…å ±
"""

import argparse
import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional


def run_trino_query(catalog: str, schema: str, query: str) -> List[List[str]]:
    """Trinoã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™"""
    cmd = [
        'docker', 'exec', 'lakehouse-trino-1', 
        'trino',
        '--catalog', catalog,
        '--schema', schema,
        '--execute', query,
        '--output-format', 'CSV_UNQUOTED'
    ]
    
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True
        )
        
        # çµæœã‚’ãƒ‘ãƒ¼ã‚¹ï¼ˆCSVãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ãï¼‰
        lines = result.stdout.strip().split('\n')
        # WARNINGãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚¹ã‚­ãƒƒãƒ—
        data_lines = [line for line in lines if not line.startswith('WARNING') and line.strip()]
        
        # CSVã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
        rows = []
        for line in data_lines:
            # ç°¡æ˜“çš„ãªCSVãƒ‘ãƒ¼ã‚¹ï¼ˆå¼•ç”¨ç¬¦ãªã—ã®å ´åˆï¼‰
            row = [field.strip() for field in line.split(',')]
            rows.append(row)
        
        return rows
    except subprocess.CalledProcessError as e:
        print(f"âŒ ã‚¯ã‚¨ãƒªå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e.stderr}", file=sys.stderr)
        return []


def get_table_list(catalog: str, schema: str) -> List[str]:
    """ã‚¹ã‚­ãƒ¼ãƒå†…ã®ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    query = "SHOW TABLES"
    rows = run_trino_query(catalog, schema, query)
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
    if rows and rows[0][0].lower() == 'table':
        rows = rows[1:]
    
    return [row[0] for row in rows if row]


def get_table_stats(catalog: str, schema: str, table: str) -> Dict[str, Any]:
    """ãƒ†ãƒ¼ãƒ–ãƒ«ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
    query = f"SHOW STATS FOR {table}"
    rows = run_trino_query(catalog, schema, query)
    
    if not rows:
        return {}
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œï¼ˆæœ€åˆã®è¡Œï¼‰
    header = rows[0]
    data_rows = rows[1:]
    
    # çµ±è¨ˆæƒ…å ±ã‚’æ§‹é€ åŒ–
    stats = {
        'table_name': table,
        'row_count': None,
        'columns': {}
    }
    
    for row in data_rows:
        if len(row) < len(header):
            continue
        
        # ã‚«ãƒ©ãƒ åãŒNULLã®è¡Œã¯å…¨ä½“çµ±è¨ˆ
        column_name = row[0]
        if not column_name or column_name.upper() == 'NULL':
            # row_countã‚’å–å¾—
            if len(row) >= 5 and row[4] and row[4] != 'NULL':
                try:
                    stats['row_count'] = float(row[4])
                except ValueError:
                    pass
            continue
        
        # ã‚«ãƒ©ãƒ çµ±è¨ˆã‚’æ§‹é€ åŒ–
        col_stats = {
            'column_name': column_name,
            'data_size': parse_float(row[1]) if len(row) > 1 else None,
            'distinct_values_count': parse_float(row[2]) if len(row) > 2 else None,
            'nulls_fraction': parse_float(row[3]) if len(row) > 3 else None,
            'low_value': row[5] if len(row) > 5 and row[5] and row[5] != 'NULL' else None,
            'high_value': row[6] if len(row) > 6 and row[6] and row[6] != 'NULL' else None,
        }
        
        stats['columns'][column_name] = col_stats
    
    return stats


def parse_float(value: str) -> Optional[float]:
    """æ–‡å­—åˆ—ã‚’floatã«å¤‰æ›ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰"""
    if not value or value.upper() == 'NULL':
        return None
    try:
        return float(value)
    except ValueError:
        return None


def main():
    parser = argparse.ArgumentParser(
        description='Trinoã‹ã‚‰ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã‚«ãƒ©ãƒ ã®çµ±è¨ˆæƒ…å ±ã‚’åé›†'
    )
    parser.add_argument(
        '--catalog',
        default='iceberg',
        help='Trinoã‚«ã‚¿ãƒ­ã‚°åï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: icebergï¼‰'
    )
    parser.add_argument(
        '--schema',
        default='imdb',
        help='ã‚¹ã‚­ãƒ¼ãƒåï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: imdbï¼‰'
    )
    parser.add_argument(
        '--tables',
        help='ãƒ†ãƒ¼ãƒ–ãƒ«åã®ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šãƒªã‚¹ãƒˆï¼ˆçœç•¥æ™‚ã¯å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰'
    )
    parser.add_argument(
        '--output-dir',
        default='datasets_statistics',
        help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: datasets_statisticsï¼‰'
    )
    
    args = parser.parse_args()
    
    print(f"ğŸ“Š Trinoçµ±è¨ˆæƒ…å ±åé›†é–‹å§‹")
    print(f"  Catalog: {args.catalog}")
    print(f"  Schema: {args.schema}")
    print()
    
    # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
    if args.tables:
        tables = [t.strip() for t in args.tables.split(',')]
        print(f"ğŸ“‹ æŒ‡å®šã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(tables)}")
    else:
        print("ğŸ“‹ å…¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—ä¸­...")
        tables = get_table_list(args.catalog, args.schema)
        print(f"   è¦‹ã¤ã‹ã£ãŸãƒ†ãƒ¼ãƒ–ãƒ«: {', '.join(tables)}")
    
    print()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    schema_dir_name = f"{args.catalog}_{args.schema}"
    output_dir = Path(args.output_dir) / schema_dir_name
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print()
    
    # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®çµ±è¨ˆæƒ…å ±ã‚’åé›†
    table_stats = {}  # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ¬ãƒ™ãƒ«ã®çµ±è¨ˆ
    column_stats = {}  # ã‚«ãƒ©ãƒ ãƒ¬ãƒ™ãƒ«ã®çµ±è¨ˆ
    
    for table in tables:
        print(f"ğŸ“Š {table} ã®çµ±è¨ˆæƒ…å ±ã‚’åé›†ä¸­...")
        stats = get_table_stats(args.catalog, args.schema, table)
        
        if stats:
            # ãƒ†ãƒ¼ãƒ–ãƒ«çµ±è¨ˆã‚’ä¿å­˜
            table_stats[table] = {
                'table_name': table,
                'row_count': stats.get('row_count'),
                'num_columns': len(stats.get('columns', {}))
            }
            
            # ã‚«ãƒ©ãƒ çµ±è¨ˆã‚’ä¿å­˜ï¼ˆtable.columnå½¢å¼ã®ã‚­ãƒ¼ï¼‰
            for col_name, col_stat in stats.get('columns', {}).items():
                full_col_name = f"{table}.{col_name}"
                column_stats[full_col_name] = {
                    'table': table,
                    'column': col_name,
                    'data_size': col_stat.get('data_size'),
                    'distinct_values_count': col_stat.get('distinct_values_count'),
                    'nulls_fraction': col_stat.get('nulls_fraction'),
                    'low_value': col_stat.get('low_value'),
                    'high_value': col_stat.get('high_value'),
                    # æ´¾ç”Ÿç‰¹å¾´
                    'avg_value_size': (
                        col_stat.get('data_size') / stats.get('row_count')
                        if col_stat.get('data_size') is not None 
                           and stats.get('row_count') 
                           and stats.get('row_count') > 0
                        else None
                    ),
                    'cardinality_ratio': (
                        col_stat.get('distinct_values_count') / stats.get('row_count')
                        if col_stat.get('distinct_values_count') is not None
                           and stats.get('row_count')
                           and stats.get('row_count') > 0
                        else None
                    )
                }
            
            # ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
            row_count = stats.get('row_count')
            num_columns = len(stats.get('columns', {}))
            print(f"   âœ… è¡Œæ•°: {int(row_count) if row_count else 'N/A'}, "
                  f"ã‚«ãƒ©ãƒ æ•°: {num_columns}")
            
            # ã‚«ãƒ©ãƒ çµ±è¨ˆã®ã‚µãƒãƒªãƒ¼
            for col_name, col_stat in stats.get('columns', {}).items():
                distinct = col_stat.get('distinct_values_count')
                nulls = col_stat.get('nulls_fraction')
                
                info_parts = []
                if distinct is not None:
                    info_parts.append(f"distinct={int(distinct)}")
                if nulls is not None:
                    info_parts.append(f"nulls={nulls:.2%}")
                
                info_str = ", ".join(info_parts) if info_parts else "no stats"
                print(f"     - {col_name}: {info_str}")
        else:
            print(f"   âŒ çµ±è¨ˆæƒ…å ±ã®å–å¾—ã«å¤±æ•—")
        
        print()
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    metadata = {
        'catalog': args.catalog,
        'schema': args.schema,
        'num_tables': len(table_stats),
        'num_columns': len(column_stats)
    }
    
    # JSONå‡ºåŠ›
    table_stats_file = output_dir / 'table_stats.json'
    column_stats_file = output_dir / 'column_stats.json'
    metadata_file = output_dir / 'metadata.json'
    
    with open(table_stats_file, 'w', encoding='utf-8') as f:
        json.dump(table_stats, f, indent=2, ensure_ascii=False)
    
    with open(column_stats_file, 'w', encoding='utf-8') as f:
        json.dump(column_stats, f, indent=2, ensure_ascii=False)
    
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ:")
    print(f"   ğŸ“„ {table_stats_file}")
    print(f"   ğŸ“„ {column_stats_file}")
    print(f"   ğŸ“„ {metadata_file}")
    print()
    print(f"ğŸ“ˆ åé›†ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«æ•°: {len(table_stats)}")
    print(f"ğŸ“ˆ åé›†ã•ã‚ŒãŸã‚«ãƒ©ãƒ æ•°: {len(column_stats)}")
    
    # ã‚µãƒãƒªãƒ¼çµ±è¨ˆ
    total_rows = sum(
        stats.get('row_count', 0) or 0 
        for stats in table_stats.values()
    )
    
    print(f"ğŸ“Š åˆè¨ˆè¡Œæ•°: {int(total_rows):,}")
    print()
    print(f"ğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print(f"   çµ±è¨ˆæƒ…å ±ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print(f"   ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ™‚ã«ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã—ã¦ãã ã•ã„")


if __name__ == '__main__':
    main()

