#!/usr/bin/env python3
"""
Trino Zero-Shot Model Training Script
çµ±åˆç‰ˆ: åŠ¹ç‡çš„ãªãƒãƒƒãƒå‡¦ç†ã¨DataLoaderå¯¾å¿œ
"""

import sys
import os
sys.path.append('src')

import argparse
import json
import re
import functools
from pathlib import Path
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset, random_split
import numpy as np
from tqdm import tqdm

from cross_db_benchmark.benchmark_tools.trino.parse_plan import parse_trino_plans
from models.zeroshot.specific_models.trino_zero_shot import TrinoZeroShotModel
from training.featurizations import TrinoTrueCardDetail
from models.zeroshot.trino_plan_batching import trino_plan_collator
from classes.classes import ZeroShotModelConfig
from training.preprocessing.feature_statistics import gather_feature_statistics, FeatureType

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®šï¼ˆå¿…é ˆï¼‰
for i in range(11):
    os.environ.setdefault(f'NODE{i:02d}', '[]')


class TrinoPlanDataset(Dataset):
    """Trinoã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ"""
    
    def __init__(self, plans):
        """
        Args:
            plans: TrinoPlanOperatorã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        self.plans = plans
    
    def __len__(self):
        return len(self.plans)
    
    def __getitem__(self, idx):
        return idx, self.plans[idx]


class MockQuery:
    """ãƒ¢ãƒƒã‚¯ã®Queryã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, plan_text):
        self.plan_text = plan_text
        self.timeout = False
        self.analyze_plans = [plan_text]  # parse_trino_plansãŒæœŸå¾…ã™ã‚‹å½¢å¼
        
        # verbose_planã¯ãƒªã‚¹ãƒˆå½¢å¼ã§æä¾›ï¼ˆparse_trino_plansãŒæœŸå¾…ã™ã‚‹å½¢å¼ï¼‰
        self.verbose_plan = plan_text.split('\n')
        
        # å®Ÿè¡Œæ™‚é–“ã‚’æŠ½å‡º
        import re
        execution_time_match = re.search(r'Execution Time: ([\d.]+)ms', plan_text)
        if execution_time_match:
            self.execution_time = float(execution_time_match.group(1))
        else:
            self.execution_time = 1000.0  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å®Ÿè¡Œæ™‚é–“ï¼ˆãƒŸãƒªç§’ï¼‰


class MockRunStats:
    """ãƒ¢ãƒƒã‚¯ã®RunStatsã‚¯ãƒ©ã‚¹ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”¨ï¼‰"""
    
    def __init__(self, plans_text):
        self.plans_text = plans_text
        # parse_trino_plansã§å¿…è¦ãªå±æ€§ã‚’è¿½åŠ 
        self.query_list = [MockQuery(plan_text) for plan_text in plans_text]
        self.database_stats = {}
    
    def __iter__(self):
        for plan_text in self.plans_text:
            yield plan_text


def split_query_plans(file_path):
    """ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã®ãƒ—ãƒ©ãƒ³ã«åˆ†å‰²"""
    with open(file_path, 'r') as f:
        content = f.read()
    
    # ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ã‚’åˆ†å‰²
    plans_text = []
    current_plan = []
    
    for line in content.split('\n'):
        if line.startswith('-- ') and 'stmt' in line and current_plan:
            plans_text.append('\n'.join(current_plan))
            current_plan = [line]
        else:
            current_plan.append(line)
    
    if current_plan:
        plans_text.append('\n'.join(current_plan))
    
    return plans_text


def load_plans_from_files(file_paths, max_plans_per_file=None):
    """
    è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ©ãƒ³ã‚’èª­ã¿è¾¼ã¿
    
    Args:
        file_paths: ãƒ—ãƒ©ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        max_plans_per_file: å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€æœ€å¤§ãƒ—ãƒ©ãƒ³æ•°
    
    Returns:
        å…¨ãƒ—ãƒ©ãƒ³ã®ãƒªã‚¹ãƒˆ
    """
    all_plans = []
    
    print(f"ğŸ“‚ {len(file_paths)}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ©ãƒ³ã‚’èª­ã¿è¾¼ã¿ä¸­...")
    
    for file_idx, file_path in enumerate(tqdm(file_paths, desc="ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿")):
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆ
        if str(file_path).endswith('.json'):
            with open(file_path, 'r') as f:
                run_data = json.load(f)
            
            # parse_trino_plansã‚’ä½¿ç”¨ã—ã¦ãƒ—ãƒ©ãƒ³ã‚’è§£æ
            # TODO: JSONã‹ã‚‰run_statsã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å†æ§‹ç¯‰
            pass
        
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼ˆEXPLAIN ANALYZEå‡ºåŠ›ï¼‰
        else:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§å€‹åˆ¥ã®ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ã«åˆ†å‰²
            with open(file_path, 'r') as f:
                content = f.read()
            
            # ã‚¯ã‚¨ãƒªãƒ—ãƒ©ãƒ³ã‚’åˆ†å‰²
            plans_text = []
            current_plan = []
            
            for line in content.split('\n'):
                if line.startswith('-- ') and 'stmt' in line and current_plan:
                    plans_text.append('\n'.join(current_plan))
                    current_plan = [line]
                else:
                    current_plan.append(line)
            
            if current_plan:
                plans_text.append('\n'.join(current_plan))
            
            # æœ€å¤§ãƒ—ãƒ©ãƒ³æ•°ã‚’åˆ¶é™
            if max_plans_per_file:
                plans_text = plans_text[:max_plans_per_file]
            
            print(f"  - {file_path}: {len(plans_text)}å€‹ã®ãƒ—ãƒ©ãƒ³ã‚’æ¤œå‡º")
            
            # MockRunStatsã‚’ä½œæˆã—ã¦ãƒ‘ãƒ¼ã‚¹
            mock_stats = MockRunStats(plans_text)
            
            parsed_runs, _ = parse_trino_plans(
                mock_stats,
                min_runtime=0,
                max_runtime=1000000,
                parse_baseline=False,
                include_zero_card=True
            )
            
            print(f"  - ãƒ‘ãƒ¼ã‚¹çµæœ: {len(parsed_runs['parsed_plans'])}å€‹ã®ãƒ—ãƒ©ãƒ³")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹IDã‚’è¨­å®š
            for plan in parsed_runs['parsed_plans']:
                plan.database_id = file_idx
            
            all_plans.extend(parsed_runs['parsed_plans'])
    
    print(f"  - ç·ãƒ—ãƒ©ãƒ³æ•°: {len(all_plans)}")
    return all_plans


def create_feature_statistics_from_plans(plans, plan_featurization, output_path=None):
    """
    ãƒ—ãƒ©ãƒ³ã‹ã‚‰ç‰¹å¾´é‡çµ±è¨ˆã‚’å‹•çš„ã«åé›†
    
    Args:
        plans: TrinoPlanOperatorã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ
        plan_featurization: ç‰¹å¾´é‡åŒ–è¨­å®š
        output_path: çµ±è¨ˆæƒ…å ±ã®å‡ºåŠ›ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    Returns:
        feature_statisticsè¾æ›¸
    """
    print("ğŸ“Š ãƒ—ãƒ©ãƒ³ã‹ã‚‰ç‰¹å¾´é‡çµ±è¨ˆã‚’åé›†ä¸­...")
    
    # ãƒ€ãƒŸãƒ¼ã®çµ±è¨ˆæƒ…å ±ã‹ã‚‰é–‹å§‹ï¼ˆå‹•çš„ã«æ‹¡å¼µï¼‰
    feature_statistics = create_dummy_feature_statistics(plan_featurization)
    
    # TODO: å®Ÿéš›ã®ãƒ—ãƒ©ãƒ³ã‹ã‚‰çµ±è¨ˆã‚’åé›†ã—ã¦æ›´æ–°
    # ã“ã®éƒ¨åˆ†ã¯ã€ã‚ˆã‚Šæ­£ç¢ºãªçµ±è¨ˆæƒ…å ±ãŒå¿…è¦ãªå ´åˆã«å®Ÿè£…
    
    if output_path:
        with open(output_path, 'w') as f:
            json.dump(feature_statistics, f, indent=2)
        print(f"  - ç‰¹å¾´é‡çµ±è¨ˆã‚’ {output_path} ã«ä¿å­˜")
    
    return feature_statistics


def create_dummy_feature_statistics(plan_featurization):
    """ãƒ€ãƒŸãƒ¼ã®ç‰¹å¾´é‡çµ±è¨ˆæƒ…å ±ã‚’ä½œæˆ"""
    feature_statistics = {}
    
    # ã™ã¹ã¦ã®ç‰¹å¾´é‡ã‚’å®šç¾©
    all_features = set()
    for features in plan_featurization.VARIABLES.values():
        all_features.update(features)
    
    for feat_name in all_features:
        if feat_name == 'op_name':
            operator_dict = {
                'Aggregate': 0, 'LocalExchange': 1, 'RemoteSource': 2, 
                'ScanFilter': 3, 'ScanFilterProject': 4, 'Project': 5, 
                'InnerJoin': 6, 'HashJoin': 7, 'NestedLoopJoin': 8,
                'Sort': 87, 'Limit': 95, 'TopN': 11,
                'TableScan': 13, 'FilterProject': 14, 'Exchange': 15,
                'LeftJoin': 32, 'ScanProject': 60, 'Filter': 61,
            }
            feature_statistics[feat_name] = {
                'type': str(FeatureType.categorical),
                'value_dict': operator_dict,
                'no_vals': 200  # å‹•çš„ã«è¿½åŠ ã•ã‚Œã‚‹æ¼”ç®—å­ã«å¯¾å¿œã™ã‚‹ãŸã‚å¤§ãã‚ã«è¨­å®š
            }
        elif feat_name == 'operator':
            operator_dict = {
                'Aggregate': 0, 'LocalExchange': 1, 'RemoteSource': 2, 
                'ScanFilter': 3, 'ScanFilterProject': 4, 'Project': 5, 
                'InnerJoin': 6, 'HashJoin': 7, 'NestedLoopJoin': 8,
                'Sort': 87, 'Limit': 95, 'TopN': 11,
                'TableScan': 13, 'FilterProject': 14, 'Exchange': 15,
                'LeftJoin': 32, 'ScanProject': 60, 'Filter': 61,
            }
            feature_statistics[feat_name] = {
                'type': str(FeatureType.categorical),
                'value_dict': operator_dict,
                'no_vals': len(operator_dict)
            }
        elif feat_name == 'aggregation':
            # é›†ç´„é–¢æ•°ã®ç‰¹å¾´é‡çµ±è¨ˆ
            aggregation_dict = {
                'Aggregator.COUNT': 0,
                'Aggregator.SUM': 1,
                'Aggregator.AVG': 2,
                'Aggregator.MIN': 3,
                'Aggregator.MAX': 4,
                None: 5  # é›†ç´„ãªã—
            }
            feature_statistics[feat_name] = {
                'type': str(FeatureType.categorical),
                'value_dict': aggregation_dict,
                'no_vals': len(aggregation_dict)
            }
        elif feat_name in ['table_name', 'column_name']:
            feature_statistics[feat_name] = {
                'type': str(FeatureType.categorical),
                'value_dict': {},
                'no_vals': 1000
            }
        elif feat_name in ['rows', 'size', 'cpu', 'memory', 'network']:
            feature_statistics[feat_name] = {
                'type': str(FeatureType.numeric),
                'mean': 0.0,
                'std': 1.0,
                'min': 0.0,
                'max': 1000000.0,
                'center': 0.0,
                'scale': 1.0
            }
        else:
            # ãã®ä»–ã®ç‰¹å¾´é‡ã¯æ•°å€¤ã¨ã—ã¦æ‰±ã†
            feature_statistics[feat_name] = {
                'type': str(FeatureType.numeric),
                'mean': 0.0,
                'std': 1.0,
                'min': 0.0,
                'max': 100.0,
                'center': 0.0,
                'scale': 1.0
            }
    
    return feature_statistics


def collect_feature_statistics(workload_run_paths, output_path):
    """
    Trinoãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã‹ã‚‰ç‰¹å¾´é‡çµ±è¨ˆã‚’åé›†
    
    Args:
        workload_run_paths: ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œçµæœã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        output_path: çµ±è¨ˆæƒ…å ±ã®å‡ºåŠ›ãƒ‘ã‚¹
    """
    print(f"ğŸ“Š ç‰¹å¾´é‡çµ±è¨ˆæƒ…å ±ã‚’åé›†ä¸­...")
    
    # gather_feature_statisticsé–¢æ•°ã‚’ä½¿ç”¨
    gather_feature_statistics(workload_run_paths, output_path)
    
    print(f"âœ… ç‰¹å¾´é‡çµ±è¨ˆæƒ…å ±ã‚’ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")


def train_epoch(model, train_loader, optimizer, device):
    """1ã‚¨ãƒãƒƒã‚¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°"""
    model.train()
    total_loss = 0
    num_batches = 0
    
    for graph, features, labels, sample_idxs in tqdm(train_loader, desc="Training"):
        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
        graph = graph.to(device)
        features = {k: v.to(device) for k, v in features.items()}
        labels_tensor = torch.tensor(labels, dtype=torch.float32, device=device).reshape(-1, 1)
        
        optimizer.zero_grad()
        
        # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
        predictions = model((graph, features))
        
        # æå¤±è¨ˆç®—
        loss = model.loss_fxn(predictions, labels_tensor)
        
        # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³
        loss.backward()
        
        # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        optimizer.step()
        
        total_loss += loss.item()
        num_batches += 1
    
    avg_loss = total_loss / num_batches if num_batches > 0 else 0
    return avg_loss


def validate(model, val_loader, device):
    """æ¤œè¨¼"""
    model.eval()
    total_loss = 0
    num_batches = 0
    predictions_all = []
    labels_all = []
    
    with torch.no_grad():
        for graph, features, labels, sample_idxs in tqdm(val_loader, desc="Validation"):
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€
            graph = graph.to(device)
            features = {k: v.to(device) for k, v in features.items()}
            labels_tensor = torch.tensor(labels, dtype=torch.float32, device=device).reshape(-1, 1)
            
            # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹
            predictions = model((graph, features))
            
            # æå¤±è¨ˆç®—
            loss = model.loss_fxn(predictions, labels_tensor)
            total_loss += loss.item()
            num_batches += 1
            
            predictions_all.append(predictions.cpu().numpy())
            labels_all.append(labels_tensor.cpu().numpy())
    
    avg_loss = total_loss / num_batches if num_batches > 0 else 0
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
    if len(predictions_all) > 0:
        predictions_all = np.concatenate(predictions_all).flatten()
        labels_all = np.concatenate(labels_all).flatten()
        
        # Q-Error
        q_errors = np.maximum(predictions_all / labels_all, labels_all / predictions_all)
        median_q_error = np.median(q_errors)
        mean_q_error = np.mean(q_errors)
        
        # RMSE
        rmse = np.sqrt(np.mean((predictions_all - labels_all) ** 2))
        
        return avg_loss, median_q_error, mean_q_error, rmse
    
    return avg_loss, None, None, None


def main():
    parser = argparse.ArgumentParser(description='Train Trino Zero-Shot Model (çµ±åˆç‰ˆ)')
    
    # ãƒ‡ãƒ¼ã‚¿é–¢é€£ã®å¼•æ•°
    parser.add_argument('--train_files', type=str, required=True,
                        help='ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šï¼‰')
    parser.add_argument('--test_file', type=str, required=True,
                        help='ãƒ†ã‚¹ãƒˆç”¨ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--statistics_file', type=str, default=None,
                        help='ç‰¹å¾´é‡çµ±è¨ˆæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰')
    
    # ãƒ¢ãƒ‡ãƒ«é–¢é€£ã®å¼•æ•°
    parser.add_argument('--output_dir', type=str, default='models/trino_zeroshot',
                        help='ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--epochs', type=int, default=100,
                        help='ã‚¨ãƒãƒƒã‚¯æ•°')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='ãƒãƒƒãƒã‚µã‚¤ã‚º')
    parser.add_argument('--lr', type=float, default=0.001,
                        help='å­¦ç¿’ç‡')
    parser.add_argument('--hidden_dim', type=int, default=128,
                        help='éš ã‚Œå±¤ã®æ¬¡å…ƒæ•°')
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu',
                        help='ãƒ‡ãƒã‚¤ã‚¹ (cuda/cpu)')
    
    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢é€£ã®å¼•æ•°
    parser.add_argument('--max_plans_per_file', type=int, default=None,
                        help='å„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€æœ€å¤§ãƒ—ãƒ©ãƒ³æ•°')
    parser.add_argument('--val_ratio', type=float, default=0.15,
                        help='æ¤œè¨¼ã‚»ãƒƒãƒˆã®å‰²åˆ')
    parser.add_argument('--num_workers', type=int, default=0,
                        help='DataLoaderã®ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°')
    
    args = parser.parse_args()
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 80)
    print("Trino Zero-Shot Model Training (çµ±åˆç‰ˆ)")
    print("=" * 80)
    print(f"Train files: {args.train_files}")
    print(f"Test file: {args.test_file}")
    print(f"Output directory: {args.output_dir}")
    print(f"Epochs: {args.epochs}")
    print(f"Batch size: {args.batch_size}")
    print(f"Learning rate: {args.lr}")
    print(f"Device: {args.device}")
    print()
    
    # 1. ãƒ—ãƒ©ãƒ³ã®èª­ã¿è¾¼ã¿
    print("ğŸ“‚ ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ—ãƒ©ãƒ³ã®èª­ã¿è¾¼ã¿")
    train_file_paths = [Path(p.strip()) for p in args.train_files.split(',')]
    test_file_path = Path(args.test_file)
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ©ãƒ³ã®èª­ã¿è¾¼ã¿
    train_plans = load_plans_from_files(train_file_paths, args.max_plans_per_file)
    
    # ãƒ†ã‚¹ãƒˆãƒ—ãƒ©ãƒ³ã®èª­ã¿è¾¼ã¿
    test_plans = load_plans_from_files([test_file_path], args.max_plans_per_file)
    
    print()
    
    # 2. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/æ¤œè¨¼ã‚»ãƒƒãƒˆã®åˆ†å‰²
    print("ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—2: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/æ¤œè¨¼ã‚»ãƒƒãƒˆã®åˆ†å‰²")
    val_size = int(len(train_plans) * args.val_ratio)
    train_size = len(train_plans) - val_size
    
    train_plans_split, val_plans_split = random_split(
        train_plans, 
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    print(f"  - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ©ãƒ³: {len(train_plans_split)}")
    print(f"  - æ¤œè¨¼ãƒ—ãƒ©ãƒ³: {len(val_plans_split)}")
    print(f"  - ãƒ†ã‚¹ãƒˆãƒ—ãƒ©ãƒ³: {len(test_plans)}")
    print()
    
    # 3. ç‰¹å¾´é‡çµ±è¨ˆã®æº–å‚™
    print("ğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—3: ç‰¹å¾´é‡çµ±è¨ˆã®æº–å‚™")
    plan_featurization = TrinoTrueCardDetail()
    
    # ç‰¹å¾´é‡çµ±è¨ˆæƒ…å ±ã®èª­ã¿è¾¼ã¿ã¾ãŸã¯ä½œæˆ
    if args.statistics_file and Path(args.statistics_file).exists():
        with open(args.statistics_file, 'r') as f:
            feature_statistics = json.load(f)
        print(f"  - æ—¢å­˜ã®çµ±è¨ˆæƒ…å ±ã‚’èª­ã¿è¾¼ã¿: {len(feature_statistics)} features")
    else:
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ—ãƒ©ãƒ³ã‹ã‚‰ç‰¹å¾´é‡çµ±è¨ˆã‚’åé›†
        feature_statistics = create_feature_statistics_from_plans(
            [train_plans[i] for i in train_plans_split.indices],
            plan_featurization,
            args.statistics_file
        )
    
    db_statistics = {}  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆã¯ç©ºï¼ˆç„¡è¦–ï¼‰
    print()
    
    # 4. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨DataLoaderã®ä½œæˆ
    print("ğŸ“¦ ã‚¹ãƒ†ãƒƒãƒ—4: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨DataLoaderã®ä½œæˆ")
    
    # collate_fnã‚’ä½œæˆï¼ˆãƒãƒƒãƒã‚’ã‚°ãƒ©ãƒ•ã«å¤‰æ›ï¼‰
    collate_fn = functools.partial(
        trino_plan_collator,
        feature_statistics=feature_statistics,
        db_statistics=db_statistics,
        plan_featurization=plan_featurization
    )
    
    # DataLoaderã®ä½œæˆ
    train_loader = DataLoader(
        TrinoPlanDataset([train_plans[i] for i in train_plans_split.indices]),
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=args.num_workers,
        collate_fn=collate_fn
    )
    
    val_loader = DataLoader(
        TrinoPlanDataset([train_plans[i] for i in val_plans_split.indices]),
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        collate_fn=collate_fn
    )
    
    test_loader = DataLoader(
        TrinoPlanDataset(test_plans),
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=args.num_workers,
        collate_fn=collate_fn
    )
    
    print(f"  - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒãƒƒãƒæ•°: {len(train_loader)}")
    print(f"  - æ¤œè¨¼ãƒãƒƒãƒæ•°: {len(val_loader)}")
    print(f"  - ãƒ†ã‚¹ãƒˆãƒãƒƒãƒæ•°: {len(test_loader)}")
    print()
    
    # 5. ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    print("ğŸ¤– ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ¢ãƒ‡ãƒ«ä½œæˆ")
    model_config = ZeroShotModelConfig(
        hidden_dim=args.hidden_dim,
        hidden_dim_plan=args.hidden_dim,
        hidden_dim_pred=args.hidden_dim,
        p_dropout=0.1,
        featurization=plan_featurization,
        output_dim=1,
        batch_size=args.batch_size
    )
    
    encoders = [
        ('plan', plan_featurization.PLAN_FEATURES),
        ('logical_pred', plan_featurization.FILTER_FEATURES),
        ('column', plan_featurization.COLUMN_FEATURES),
        ('table', plan_featurization.TABLE_FEATURES),
        ('filter_column', plan_featurization.FILTER_FEATURES + plan_featurization.COLUMN_FEATURES),
        ('output_column', plan_featurization.OUTPUT_COLUMN_FEATURES)
    ]
    
    model = TrinoZeroShotModel(
        model_config=model_config,
        device=args.device,
        feature_statistics=feature_statistics,
        plan_featurization=plan_featurization,
        add_tree_model_types=[],
        encoders=encoders
    )
    
    model = model.to(args.device)
    
    print(f"  - ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print()
    
    # 6. ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
    print("âš™ï¸  ã‚¹ãƒ†ãƒƒãƒ—6: ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶è¨­å®š")
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=10, verbose=True
    )
    print(f"  - Optimizer: Adam (lr={args.lr})")
    print(f"  - Scheduler: ReduceLROnPlateau")
    print()
    
    # 7. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—
    print("ğŸš€ ã‚¹ãƒ†ãƒƒãƒ—7: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹")
    best_val_loss = float('inf')
    best_epoch = 0
    
    for epoch in range(args.epochs):
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
        train_loss = train_epoch(model, train_loader, optimizer, args.device)
        
        # æ¤œè¨¼
        val_result = validate(model, val_loader, args.device)
        val_loss, val_median_q_error, val_mean_q_error, val_rmse = val_result
        
        # å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
        scheduler.step(val_loss)
        
        # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch + 1
            # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜
            torch.save(model.state_dict(), output_dir / 'best_model.pt')
        
        # ãƒ­ã‚°å‡ºåŠ›
        if (epoch + 1) % 5 == 0 or epoch == 0:
            print(f"Epoch [{epoch+1}/{args.epochs}] "
                  f"Train Loss: {train_loss:.4f}, "
                  f"Val Loss: {val_loss:.4f}, "
                  f"Val Q-Error: {val_median_q_error:.4f}, "
                  f"Best: {best_val_loss:.4f} (Epoch {best_epoch})")
    
    print()
    print("âœ… ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†!")
    print()
    
    # 8. ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®è©•ä¾¡
    print("ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—8: ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã®æœ€çµ‚è©•ä¾¡")
    
    # ãƒ™ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    model.load_state_dict(torch.load(output_dir / 'best_model.pt'))
    test_result = validate(model, test_loader, args.device)
    test_loss, test_median_q_error, test_mean_q_error, test_rmse = test_result
    
    print(f"ã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆè©•ä¾¡çµæœã€‘")
    print(f"  - ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(test_plans)}")
    print(f"  - Test Loss: {test_loss:.4f}")
    print(f"  - RMSE: {test_rmse:.4f}ç§’ ({test_rmse*1000:.2f}ms)")
    print(f"  - Median Q-Error: {test_median_q_error:.4f}")
    print(f"  - Mean Q-Error: {test_mean_q_error:.4f}")
    print()
    
    print("=" * 80)
    print("ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†ï¼")
    print(f"Best Validation Loss: {best_val_loss:.4f} (Epoch {best_epoch})")
    print(f"Test Median Q-Error: {test_median_q_error:.4f}")
    print(f"Model saved to: {output_dir / 'best_model.pt'}")
    print("=" * 80)


if __name__ == "__main__":
    main()

